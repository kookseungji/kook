{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: glove==1.0.0 in c:\\programdata\\anaconda3\\envs\\study\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: cython in c:\\programdata\\anaconda3\\envs\\study\\lib\\site-packages (from glove==1.0.0) (0.29.13)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\study\\lib\\site-packages (from glove==1.0.0) (1.16.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install glove==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_vectors():\n",
    "    vectors = {}\n",
    "    f=open(\"glove.6B.50d.txt\",encoding='utf-8')\n",
    "    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck']\n",
    "    for i in f:\n",
    "        word = i.split()[0]\n",
    "        if word in classes:\n",
    "            vectors[word] = i.split()[1:]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "x = tf.placeholder(tf.float32, shape=[None, 32,32,3])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "lr = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "W_conv1 = weight_variable([3, 3, 3, 16])\n",
    "b_conv1 = bias_variable([16])\n",
    "h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([3, 3, 16, 32])\n",
    "b_conv2 = bias_variable([32])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "W_conv3 = weight_variable([3, 3, 32, 128])\n",
    "b_conv3 = bias_variable([128])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3) + b_conv3)\n",
    "\n",
    "W_conv4 = weight_variable([3, 3, 128, 256])\n",
    "b_conv4 = bias_variable([256])\n",
    "h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4) + b_conv4)\n",
    "h_pool4 = max_pool_2x2(h_conv4)\n",
    "\n",
    "W_fc1 = weight_variable([8 * 8 * 256, 4096])\n",
    "b_fc1 = bias_variable([4096])\n",
    "\n",
    "h_pool4_flat = tf.reshape(h_pool4, [-1, 8*8*256])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool4_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([4096, 4096])\n",
    "b_fc2 = bias_variable([4096])\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "W_fc3 = weight_variable([4096, 10])\n",
    "b_fc3 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)), reduction_indices=[1]))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# sess.run(tf.initialize_all_variables())\n",
    "\n",
    "\n",
    "def train(X_train, Y_train, X_validation, Y_validation):\n",
    "    train_tuple = list(zip(X_train, Y_train))\n",
    "\n",
    "    for i in range(16000):\n",
    "\n",
    "        batch = random.sample(train_tuple, 32)\n",
    "        batch_X = [j[0] for j in batch]\n",
    "        batch_Y = [j[1] for j in batch]\n",
    "        if i%1000==0:\n",
    "                va = 0\n",
    "                for j in range(0, len(X_train), 32):\n",
    "                    mx = min(j+32, len(X_train))\n",
    "                    va = va + (accuracy.eval(feed_dict={x: X_train[j:mx], y_: Y_train[j:mx], keep_prob: 1.0}))*(mx-j)\n",
    "                va /= len(X_train)\n",
    "                print (\"train\", va)\n",
    "\n",
    "                va = 0\n",
    "                for j in range(0, len(X_validation), 32):\n",
    "                    mx = min(j+32, len(X_validation))\n",
    "                    va = va + (accuracy.eval(feed_dict={x: X_validation[j:mx], y_: Y_validation[j:mx], keep_prob: 1.0}))*(mx-j)\n",
    "                va /= len(X_validation)\n",
    "                print (\"validation\", va)\n",
    "\n",
    "        if i%10 == 0 and i!=0:\n",
    "            print (\"step\", i, \"loss\", loss_val)\n",
    "\n",
    "        if i<4000:\n",
    "            _, loss_val = sess.run([train_step, cross_entropy], feed_dict={x:batch_X, y_: batch_Y, keep_prob: 0.5, lr: 2e-4})\n",
    "        else:\n",
    "            _, loss_val = sess.run([train_step, cross_entropy], feed_dict={x:batch_X, y_: batch_Y, keep_prob: 0.5, lr: 2e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# with tf.device('/gpu:0'):\n",
    "x = tf.placeholder(tf.float32, shape=[None, 32,32,3])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 50])\n",
    "lr = tf.placeholder(tf.float32)\n",
    "\n",
    "W_conv1 = weight_variable([3, 3, 3, 16])\n",
    "b_conv1 = bias_variable([16])\n",
    "h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([3, 3, 16, 32])\n",
    "b_conv2 = bias_variable([32])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "W_conv3 = weight_variable([3, 3, 32, 128])\n",
    "b_conv3 = bias_variable([128])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3) + b_conv3)\n",
    "\n",
    "W_conv4 = weight_variable([3, 3, 128, 256])\n",
    "b_conv4 = bias_variable([256])\n",
    "h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4) + b_conv4)\n",
    "h_pool4 = max_pool_2x2(h_conv4)\n",
    "\n",
    "W_fc1 = weight_variable([8 * 8 * 256, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool4_flat = tf.reshape(h_pool4, [-1, 8*8*256])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool4_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 1024])\n",
    "b_fc2 = bias_variable([1024])\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "W_fc3 = weight_variable([1024, 50])\n",
    "b_fc3 = bias_variable([50])\n",
    "\n",
    "y_conv= tf.matmul(h_fc2, W_fc3) + b_fc3\n",
    "\n",
    "loss = tf.nn.l2_loss(y_conv-y_)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "# sess =  tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# sess.run(tf.initialize_all_variables())\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def train(X_train, Y_train):\n",
    "    train_tuple = list(zip(X_train, Y_train))\n",
    "\n",
    "    for i in range(10000):\n",
    "        batch = random.sample(train_tuple, 32)\n",
    "        batch_X = [j[0] for j in batch]\n",
    "        batch_Y = [j[1] for j in batch]\n",
    "\n",
    "        if i%10 == 0 and i!=0:\n",
    "            print (\"step\", i, \"loss\", loss_val)\n",
    "\n",
    "        if i<10000:\n",
    "            rate = 2e-4\n",
    "        else:\n",
    "            rate = 2e-5\n",
    "\n",
    "        _, loss_val = sess.run([train_step, loss], feed_dict={x:batch_X, y_: batch_Y, keep_prob: 0.8, lr: rate})\n",
    "\n",
    "def predict(X):\n",
    "    prediction = sess.run([y_conv], feed_dict={x: X, keep_prob: 1.0})\n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 loss 206.04593\n",
      "step 20 loss 180.14911\n",
      "step 30 loss 188.85385\n",
      "step 40 loss 175.6188\n",
      "step 50 loss 193.7104\n",
      "step 60 loss 173.74283\n",
      "step 70 loss 197.5463\n",
      "step 80 loss 189.62863\n",
      "step 90 loss 178.2156\n",
      "step 100 loss 162.20049\n",
      "step 110 loss 181.13248\n",
      "step 120 loss 153.04572\n",
      "step 130 loss 159.02278\n",
      "step 140 loss 162.08073\n",
      "step 150 loss 156.61676\n",
      "step 160 loss 136.52077\n",
      "step 170 loss 124.33312\n",
      "step 180 loss 135.17001\n",
      "step 190 loss 160.25082\n",
      "step 200 loss 153.0314\n",
      "step 210 loss 138.13034\n",
      "step 220 loss 154.17618\n",
      "step 230 loss 145.70187\n",
      "step 240 loss 105.70745\n",
      "step 250 loss 142.7944\n",
      "step 260 loss 139.43703\n",
      "step 270 loss 128.1754\n",
      "step 280 loss 102.67488\n",
      "step 290 loss 113.45326\n",
      "step 300 loss 133.9789\n",
      "step 310 loss 137.42917\n",
      "step 320 loss 157.5947\n",
      "step 330 loss 163.36536\n",
      "step 340 loss 162.05304\n",
      "step 350 loss 128.27884\n",
      "step 360 loss 124.23118\n",
      "step 370 loss 121.91829\n",
      "step 380 loss 129.15547\n",
      "step 390 loss 124.81131\n",
      "step 400 loss 121.432465\n",
      "step 410 loss 133.05612\n",
      "step 420 loss 103.9955\n",
      "step 430 loss 151.62836\n",
      "step 440 loss 142.90572\n",
      "step 450 loss 114.28041\n",
      "step 460 loss 137.99637\n",
      "step 470 loss 111.85115\n",
      "step 480 loss 113.5503\n",
      "step 490 loss 137.49403\n",
      "step 500 loss 115.101974\n",
      "step 510 loss 124.40143\n",
      "step 520 loss 129.06197\n",
      "step 530 loss 102.396835\n",
      "step 540 loss 141.60718\n",
      "step 550 loss 94.92949\n",
      "step 560 loss 124.06836\n",
      "step 570 loss 119.16301\n",
      "step 580 loss 103.75251\n",
      "step 590 loss 108.48335\n",
      "step 600 loss 126.43487\n",
      "step 610 loss 103.225845\n",
      "step 620 loss 131.04959\n",
      "step 630 loss 115.14169\n",
      "step 640 loss 119.8376\n",
      "step 650 loss 146.36073\n",
      "step 660 loss 102.72141\n",
      "step 670 loss 118.91016\n",
      "step 680 loss 145.43335\n",
      "step 690 loss 108.501945\n",
      "step 700 loss 118.9796\n",
      "step 710 loss 120.465515\n",
      "step 720 loss 143.71014\n",
      "step 730 loss 122.50339\n",
      "step 740 loss 127.60355\n",
      "step 750 loss 111.765015\n",
      "step 760 loss 103.71848\n",
      "step 770 loss 110.468704\n",
      "step 780 loss 102.0922\n",
      "step 790 loss 99.73785\n",
      "step 800 loss 94.21123\n",
      "step 810 loss 115.090515\n",
      "step 820 loss 104.8873\n",
      "step 830 loss 97.98688\n",
      "step 840 loss 100.19943\n",
      "step 850 loss 93.40006\n",
      "step 860 loss 103.16137\n",
      "step 870 loss 133.57396\n",
      "step 880 loss 106.77417\n",
      "step 890 loss 96.62409\n",
      "step 900 loss 116.825165\n",
      "step 910 loss 119.96006\n",
      "step 920 loss 128.69624\n",
      "step 930 loss 83.63214\n",
      "step 940 loss 78.43102\n",
      "step 950 loss 123.3103\n",
      "step 960 loss 94.56097\n",
      "step 970 loss 113.47751\n",
      "step 980 loss 101.624466\n",
      "step 990 loss 89.51667\n",
      "step 1000 loss 94.01771\n",
      "step 1010 loss 114.46272\n",
      "step 1020 loss 128.47858\n",
      "step 1030 loss 109.25099\n",
      "step 1040 loss 108.12463\n",
      "step 1050 loss 95.17353\n",
      "step 1060 loss 104.66234\n",
      "step 1070 loss 95.45748\n",
      "step 1080 loss 75.28933\n",
      "step 1090 loss 82.28998\n",
      "step 1100 loss 80.125046\n",
      "step 1110 loss 141.41347\n",
      "step 1120 loss 85.11458\n",
      "step 1130 loss 94.32568\n",
      "step 1140 loss 137.1896\n",
      "step 1150 loss 87.6916\n",
      "step 1160 loss 108.19052\n",
      "step 1170 loss 119.47133\n",
      "step 1180 loss 86.12593\n",
      "step 1190 loss 90.83224\n",
      "step 1200 loss 81.29983\n",
      "step 1210 loss 74.09827\n",
      "step 1220 loss 111.061264\n",
      "step 1230 loss 74.012474\n",
      "step 1240 loss 82.14281\n",
      "step 1250 loss 113.085106\n",
      "step 1260 loss 88.29582\n",
      "step 1270 loss 128.85556\n",
      "step 1280 loss 78.092255\n",
      "step 1290 loss 74.9884\n",
      "step 1300 loss 103.97346\n",
      "step 1310 loss 84.62636\n",
      "step 1320 loss 81.03783\n",
      "step 1330 loss 92.71416\n",
      "step 1340 loss 62.640747\n",
      "step 1350 loss 88.28715\n",
      "step 1360 loss 95.88379\n",
      "step 1370 loss 61.972\n",
      "step 1380 loss 77.61908\n",
      "step 1390 loss 100.76254\n",
      "step 1400 loss 77.99943\n",
      "step 1410 loss 71.496475\n",
      "step 1420 loss 80.72632\n",
      "step 1430 loss 91.96903\n",
      "step 1440 loss 103.357025\n",
      "step 1450 loss 92.596344\n",
      "step 1460 loss 62.50627\n",
      "step 1470 loss 95.01154\n",
      "step 1480 loss 89.790535\n",
      "step 1490 loss 85.251755\n",
      "step 1500 loss 103.23117\n",
      "step 1510 loss 87.1221\n",
      "step 1520 loss 80.05879\n",
      "step 1530 loss 93.34302\n",
      "step 1540 loss 97.88788\n",
      "step 1550 loss 75.863205\n",
      "step 1560 loss 86.1708\n",
      "step 1570 loss 76.940186\n",
      "step 1580 loss 88.38522\n",
      "step 1590 loss 80.98159\n",
      "step 1600 loss 57.380417\n",
      "step 1610 loss 99.12417\n",
      "step 1620 loss 86.62161\n",
      "step 1630 loss 83.521576\n",
      "step 1640 loss 89.89894\n",
      "step 1650 loss 87.451706\n",
      "step 1660 loss 72.50317\n",
      "step 1670 loss 77.925865\n",
      "step 1680 loss 82.27408\n",
      "step 1690 loss 74.59326\n",
      "step 1700 loss 89.47978\n",
      "step 1710 loss 56.05442\n",
      "step 1720 loss 87.02164\n",
      "step 1730 loss 84.769455\n",
      "step 1740 loss 80.62727\n",
      "step 1750 loss 68.33055\n",
      "step 1760 loss 60.81446\n",
      "step 1770 loss 68.31914\n",
      "step 1780 loss 58.023346\n",
      "step 1790 loss 79.705154\n",
      "step 1800 loss 67.00522\n",
      "step 1810 loss 67.76793\n",
      "step 1820 loss 95.18768\n",
      "step 1830 loss 108.42095\n",
      "step 1840 loss 119.44008\n",
      "step 1850 loss 88.942825\n",
      "step 1860 loss 69.81297\n",
      "step 1870 loss 51.560516\n",
      "step 1880 loss 71.13896\n",
      "step 1890 loss 75.43091\n",
      "step 1900 loss 69.10593\n",
      "step 1910 loss 68.37477\n",
      "step 1920 loss 58.77052\n",
      "step 1930 loss 59.52799\n",
      "step 1940 loss 76.21879\n",
      "step 1950 loss 86.104126\n",
      "step 1960 loss 60.737373\n",
      "step 1970 loss 88.050934\n",
      "step 1980 loss 107.68054\n",
      "step 1990 loss 45.93712\n",
      "step 2000 loss 61.719772\n",
      "step 2010 loss 56.87043\n",
      "step 2020 loss 67.98543\n",
      "step 2030 loss 84.80815\n",
      "step 2040 loss 43.112854\n",
      "step 2050 loss 70.35835\n",
      "step 2060 loss 79.8173\n",
      "step 2070 loss 95.86871\n",
      "step 2080 loss 59.59758\n",
      "step 2090 loss 84.32502\n",
      "step 2100 loss 98.116325\n",
      "step 2110 loss 70.85532\n",
      "step 2120 loss 101.583496\n",
      "step 2130 loss 60.777275\n",
      "step 2140 loss 62.1764\n",
      "step 2150 loss 72.015205\n",
      "step 2160 loss 62.68645\n",
      "step 2170 loss 90.7486\n",
      "step 2180 loss 84.00285\n",
      "step 2190 loss 71.57904\n",
      "step 2200 loss 57.658638\n",
      "step 2210 loss 46.76326\n",
      "step 2220 loss 82.02748\n",
      "step 2230 loss 69.1009\n",
      "step 2240 loss 79.66562\n",
      "step 2250 loss 45.556694\n",
      "step 2260 loss 56.994606\n",
      "step 2270 loss 62.99588\n",
      "step 2280 loss 102.99066\n",
      "step 2290 loss 89.32924\n",
      "step 2300 loss 73.167694\n",
      "step 2310 loss 75.61772\n",
      "step 2320 loss 109.008316\n",
      "step 2330 loss 76.26048\n",
      "step 2340 loss 71.38649\n",
      "step 2350 loss 55.152348\n",
      "step 2360 loss 81.84732\n",
      "step 2370 loss 64.92176\n",
      "step 2380 loss 80.70468\n",
      "step 2390 loss 50.0654\n",
      "step 2400 loss 80.39896\n",
      "step 2410 loss 63.82451\n",
      "step 2420 loss 84.36801\n",
      "step 2430 loss 118.765335\n",
      "step 2440 loss 74.72313\n",
      "step 2450 loss 54.23721\n",
      "step 2460 loss 54.800404\n",
      "step 2470 loss 63.96168\n",
      "step 2480 loss 80.949425\n",
      "step 2490 loss 58.93544\n",
      "step 2500 loss 58.542236\n",
      "step 2510 loss 75.39849\n",
      "step 2520 loss 89.13672\n",
      "step 2530 loss 47.720695\n",
      "step 2540 loss 54.94909\n",
      "step 2550 loss 91.079575\n",
      "step 2560 loss 87.99194\n",
      "step 2570 loss 75.4577\n",
      "step 2580 loss 110.46053\n",
      "step 2590 loss 69.512955\n",
      "step 2600 loss 52.07925\n",
      "step 2610 loss 70.936005\n",
      "step 2620 loss 69.383705\n",
      "step 2630 loss 74.120316\n",
      "step 2640 loss 78.909004\n",
      "step 2650 loss 118.69943\n",
      "step 2660 loss 66.035065\n",
      "step 2670 loss 87.19206\n",
      "step 2680 loss 104.093796\n",
      "step 2690 loss 57.686287\n",
      "step 2700 loss 69.72427\n",
      "step 2710 loss 70.45304\n",
      "step 2720 loss 46.887688\n",
      "step 2730 loss 24.973679\n",
      "step 2740 loss 59.719826\n",
      "step 2750 loss 76.37456\n",
      "step 2760 loss 68.10182\n",
      "step 2770 loss 61.284027\n",
      "step 2780 loss 60.5883\n",
      "step 2790 loss 53.817917\n",
      "step 2800 loss 59.247032\n",
      "step 2810 loss 73.84813\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import word_vec\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import regression_based\n",
    "from numpy import array\n",
    "from word_vec import get_vectors\n",
    "def unpickle(file):  \n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "embeddings = word_vec.get_vectors()\n",
    "\n",
    "train_d = unpickle('cifar-10-batches-py/data_batch_1')\n",
    "X = train_d['data']\n",
    "Y = train_d['labels']\n",
    "train_d = unpickle('cifar-10-batches-py/data_batch_2')\n",
    "X = np.vstack((X,train_d['data']))\n",
    "Y = Y + train_d['labels']\n",
    "train_d = unpickle('cifar-10-batches-py/data_batch_3')\n",
    "X = np.vstack((X,train_d['data']))\n",
    "Y = Y + train_d['labels']\n",
    "train_d = unpickle('cifar-10-batches-py/data_batch_4')\n",
    "X = np.vstack((X,train_d['data']))\n",
    "Y = Y + train_d['labels']\n",
    "train_d = unpickle('cifar-10-batches-py/data_batch_5')\n",
    "X = np.vstack((X,train_d['data']))\n",
    "Y = Y + train_d['labels']\n",
    "\n",
    "X = np.reshape(X, (50000,3,32,32)).transpose(0,2,3,1)\n",
    "\n",
    "X_train = X[:40000]\n",
    "Y_train = Y[:40000]\n",
    "X_validation  = X[40000:]\n",
    "Y_validation = Y[40000:]\n",
    "\n",
    "\n",
    "def classification_baseline():\n",
    "    import baseline\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    Y_train = enc.fit_transform(np.array(Y_train).reshape(-1,1))\n",
    "    Y_validation = enc.fit_transform(np.array(Y_validation).reshape(-1,1))\n",
    "    baseline.train(X_train, Y_train, X_validation, Y_validation)\n",
    "\n",
    "def classify_embedding():\n",
    "    import classification_based\n",
    "    class_labels = unpickle('cifar-10-batches-py/batches.meta')['label_names']\n",
    "    # print class_labels\n",
    "    Y_8_train = np.array(Y_train)\n",
    "    X_8_train = np.array(X_train)\n",
    "\n",
    "    removed_indices = np.where(Y_8_train!=8)\n",
    "    Y_8_train = Y_8_train[removed_indices]\n",
    "    X_8_train = X_8_train[removed_indices]\n",
    "    removed_indices = np.where(Y_8_train!=9)\n",
    "    Y_8_train = Y_8_train[removed_indices]\n",
    "    X_8_train = X_8_train[removed_indices]\n",
    "\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    Y_8_train = enc.fit_transform(np.array(Y_8_train).reshape(-1,1))\n",
    "\n",
    "    Y_8_validation = np.array(Y_validation)\n",
    "    X_8_validation = np.array(X_validation)\n",
    "\n",
    "    removed_indices = np.where(Y_8_validation!=8)\n",
    "    Y_8_validation = Y_8_validation[removed_indices]\n",
    "    X_8_validation = X_8_validation[removed_indices]\n",
    "    removed_indices = np.where(Y_8_validation!=9)\n",
    "    Y_8_validation = Y_8_validation[removed_indices]\n",
    "    X_8_validation = X_8_validation[removed_indices]\n",
    "\n",
    "    Y_8_validation = enc.fit_transform(np.array(Y_8_validation).reshape(-1,1))\n",
    "\n",
    "    classification_based.train(X_8_train, Y_8_train, X_8_validation, Y_8_validation)\n",
    "\n",
    "    Y_2_validation = np.array(Y_validation)\n",
    "    X_2_validation = np.array(X_validation)\n",
    "\n",
    "    indices = np.where(Y_2_validation>=8)\n",
    "    Y_2_validation = Y_2_validation[indices]\n",
    "    X_2_validation = X_2_validation[indices]\n",
    "\n",
    "    validaiton_probab = classification_based.predict_probabilites(X_2_validation)\n",
    "    weights = []\n",
    "    for i in class_labels[:-2]:\n",
    "        weights.append(embeddings[i])\n",
    "    weights = np.array(weights, dtype=np.float32)\n",
    "\n",
    "    validaiton_embeddings = np.dot(validaiton_probab, weights)\n",
    "\n",
    "    targets_embeddings = []\n",
    "    for i in class_labels:\n",
    "        targets_embeddings.append(embeddings[i])\n",
    "    targets_embeddings = np.array(targets_embeddings, dtype=np.float32)\n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from scipy.spatial.distance import cosine\n",
    "    Y_pred_validation = []\n",
    "    for i in validaiton_embeddings:\n",
    "        cos = []\n",
    "        for j in targets_embeddings:\n",
    "            val = cosine(i,j)\n",
    "            cos.append(val)\n",
    "        Y_pred_validation.append(np.argmax(cos))\n",
    "\n",
    "    # neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "    # neigh.fit(targets_embeddings, [0,1,2,3,4,5,6,7,8,9])\n",
    "    # Y_pred_validation = neigh.predict(validaiton_embeddings)\n",
    "    # print Y_2_validation\n",
    "    # print Y_pred_validation\n",
    "    print (accuracy_score(Y_2_validation, Y_pred_validation))\n",
    "\n",
    "    for i,j in zip(Y_2_validation, Y_pred_validation):\n",
    "        print (i,j)\n",
    "\n",
    "def regression_embedding():\n",
    "    import regression_based\n",
    "    class_labels = unpickle('cifar-10-batches-py/batches.meta')['label_names']\n",
    "    # print class_labels\n",
    "    Y_8_train = np.array(Y_train)\n",
    "    X_8_train = np.array(X_train)\n",
    "\n",
    "    removed_indices = np.where(Y_8_train!=1)\n",
    "    Y_8_train = Y_8_train[removed_indices]\n",
    "    X_8_train = X_8_train[removed_indices]\n",
    "    removed_indices = np.where(Y_8_train!=4)\n",
    "    Y_8_train = Y_8_train[removed_indices]\n",
    "    X_8_train = X_8_train[removed_indices]\n",
    "\n",
    "    Y_8_train = [embeddings[class_labels[i]] for i in Y_8_train]\n",
    "\n",
    "    Y_8_validation = np.array(Y_validation)\n",
    "    X_8_validation = np.array(X_validation)\n",
    "\n",
    "\n",
    "    regression_based.train(X_8_train, Y_8_train)\n",
    "\n",
    "    Y_2_validation = np.array(Y_validation)\n",
    "    X_2_validation = np.array(X_validation)\n",
    "\n",
    "    indices = np.where(np.logical_or(Y_2_validation==1 ,Y_2_validation==4))\n",
    "    Y_2_validation = Y_2_validation[indices]\n",
    "    X_2_validation = X_2_validation[indices]\n",
    "\n",
    "    validaiton_embeddings = regression_based.predict(X_2_validation)\n",
    "\n",
    "    targets_embeddings = []\n",
    "    for i in ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck']:\n",
    "        targets_embeddings.append(embeddings[i])\n",
    "    targets_embeddings = np.array(targets_embeddings, dtype=np.float32)\n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from scipy.spatial.distance import cosine\n",
    "    # Y_pred_validation = []\n",
    "    # for i in validaiton_embeddings:\n",
    "    #     cos = []\n",
    "    #     for j in targets_embeddings:\n",
    "    #         val = cosine(i,j)\n",
    "    #         cos.append(val)\n",
    "    #     Y_pred_validation.append(np.argmax(cos))\n",
    "    #\n",
    "    neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "    neigh.fit(targets_embeddings, [0,1,2,3,4,5,6,7,8,9])\n",
    "    Y_pred_validation = neigh.predict(validaiton_embeddings)\n",
    "    print(Y_2_validation)\n",
    "    print(Y_pred_validation)\n",
    "    print (accuracy_score(Y_2_validation, Y_pred_validation))\n",
    "\n",
    "\n",
    "    # data = np.vstack((validaiton_embeddings, targets_embeddings))\n",
    "    # from sklearn import manifold\n",
    "    # tsne = manifold.TSNE(n_components=2)\n",
    "    # X_tsne = tsne.fit_transform(data)\n",
    "    #\n",
    "    # print X_tsne.shape\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # Y_2_validation = Y_pred_validation.tolist()  + [0,1,2,3,4,5,6,7,8,9]\n",
    "    # plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=Y_2_validation, cmap=plt.cm.get_cmap(\"jet\", 10))\n",
    "    # plt.colorbar(ticks=range(10))\n",
    "    # plt.clim(-0.5, 9.5)\n",
    "    # plt.show()\n",
    "\n",
    "regression_embedding()\n",
    "\n",
    "vec = get_vectors()\n",
    "\n",
    "Y_8_train = np.array(Y_train)\n",
    "X_8_train = np.array(X_train)\n",
    "\n",
    "removed_indices = np.where(Y_8_train!=1)\n",
    "Y_8_train = Y_8_train[removed_indices]\n",
    "X_8_train = X_8_train[removed_indices]\n",
    "removed_indices = np.where(Y_8_train!=4)\n",
    "Y_8_train = Y_8_train[removed_indices]\n",
    "X_8_train = X_8_train[removed_indices]\n",
    "\n",
    "Y_2_validation = np.array(Y)\n",
    "X_2_validation = np.array(X)\n",
    "\n",
    "indices = np.where(np.logical_or(Y_2_validation == 1, Y_2_validation == 4))\n",
    "Y_2_validation = Y_2_validation[indices]\n",
    "X_2_validation = X_2_validation[indices]\n",
    "\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck']\n",
    "\n",
    "class_list = [vec[i] for i in classes]\n",
    "# print(class_list)\n",
    "a = regression_based.predict(np.array(X_train)[:20000])  # 예측부\n",
    "\n",
    "pca = PCA(2)\n",
    "class_list.extend(a)\n",
    "total_list = pca.fit_transform(class_list)\n",
    "\n",
    "tx = [row for row, col in total_list[:10]]\n",
    "ty = [col for row, col in total_list[:10]]\n",
    "x_t=[]\n",
    "y_t=[]\n",
    "from matplotlib.font_manager import FontProperties\n",
    "for j in range(10):\n",
    "    x_t.append([row for i,(row, col) in enumerate(total_list[10:]) if Y_train[i] == j])  #위의 예측부를 바꿔줬다면 여기도 수정해야함 ex) X_train->X_8_train으로 바궛다면 Y_8_train[i]로\n",
    "    y_t.append([col for i, (row, col) in enumerate(total_list[10:]) if Y_train[i] == j]) #위의 예측부를 바꿔줬다면 여기도 수정해야함\n",
    "\n",
    "marker = ['.','o','v','^','>','<','s','p','*','h']   #마커랑 컬러 바꾸고싶다면..여기를 수정\n",
    "colors = ['g','b','c','y','m','g','b','c','y','m']\n",
    "for j in range(10):\n",
    "    plt.scatter(x_t[j],y_t[j],marker=marker[j],color = colors[j],label=classes[j])\n",
    "fontP = FontProperties()\n",
    "fontP.set_size('small')\n",
    "plt.legend(bbox_to_anchor=(0,1), loc='upper right',prop=fontP)\n",
    "# plt.scatter(x_1,y_1,color='b')\n",
    "# plt.scatter(x_4,y_4, color = 'y')\n",
    "plt.scatter(tx,ty,color='r')\n",
    "for i, txt in enumerate(classes):\n",
    "    plt.annotate(txt,(tx[i],ty[i]))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
