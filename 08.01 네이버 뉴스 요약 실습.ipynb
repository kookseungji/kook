{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'statementls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f2a9e49732cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosineSimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m \u001b[0msimilarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextSimilarityExample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatementls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'statementls' is not defined"
     ]
    }
   ],
   "source": [
    "import math \n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class TextSimilarityExample:\n",
    "    def __init__(self, statementls):\n",
    "        statementls = ['ruled india', 'Chalukyas ruled Badami','So many kingdoms ruled India',\n",
    "'Lalbagh is a botanical garden in India']\n",
    "        self.statements = statementsls    \n",
    "    \n",
    "    def TF(self, sentence):\n",
    "        for sentence in sentences:\n",
    "            bowA=sentence.split()\n",
    "            bowB=sentence.split()\n",
    "            bowC=sentence.split()\n",
    "            bowD=sentence.split()\n",
    "            wordSet = set(bowA).union(set(bowB)).union(set(bowC)).union(set(bowD))\n",
    "            wordDictA = dict.fromkeys(wordSet, 0) \n",
    "            wordDictB = dict.fromkeys(wordSet, 0) \n",
    "            for word in bowA:\n",
    "                wordDictA[word]+=1\n",
    "    \n",
    "            for word in bowB:\n",
    "                wordDictB[word]+=1\n",
    "       \n",
    "            pd.DataFrame([wordDictA, wordDictB])\n",
    "            \n",
    "        dictionary = {}\n",
    "        bowCount = len(bow)\n",
    "        for word, count in wordDict.items():\n",
    "             tfDict[word] = count/float(bowCount)\n",
    "                \n",
    "        tfBowA = computeTF(wordDictA, bowA)\n",
    "        tfBowB = computeTF(wordDictB, bowB)    \n",
    "            \n",
    "        return dictionary\n",
    "        \n",
    "    def IDF(self):\n",
    "        \n",
    "        idfDict = {}\n",
    "        N = len(docList)\n",
    "\n",
    "        idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "        for doc in docList:\n",
    "            for word, val in doc.items():\n",
    "                if val > 0:\n",
    "                    idfDict[word] += 1\n",
    "\n",
    "        for word, val in idfDict.items():\n",
    "            idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "        return idfDict\n",
    "        idfs = computeIDF([wordDictA, wordDictB])\n",
    "        \n",
    "    def TF_IDF(self, query):\n",
    "        tfidf = {}\n",
    "        for word, val in tfBow.items():\n",
    "            tfidf[word] = val*idfs[word]\n",
    "        return vectors\n",
    "        \n",
    "        tfidfBowA = computeTFIDF(tfBowA, idfs)\n",
    "        tfidfBowB = computeTFIDF(tfBowB, idfs)\n",
    "    def displayVectors(self, vectors):\n",
    "    \n",
    "        return\n",
    "    \n",
    "    def cosineSimilarity(x, y):\n",
    "        nominator=np.dot(x,y)\n",
    "        denominator=np.linalg.norm(x)*np.linalg.norm(y)\n",
    "        return nominator/denominator\n",
    "    \n",
    "    def demo(self):\n",
    "        inputQuery = self.statements[0]\n",
    "        vectors = self.TF_IDF(inputQuery)\n",
    "        self.displayVectors(vectors)\n",
    "        self.cosineSimilarity(vectors[0], vectors[1])\n",
    "\n",
    "similarity = TextSimilarityExample(statementls)\n",
    "similarity.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from operator import itemgetter   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=101&oid=001&aid=0010995958'\n",
    "resp=requests.get(url)\n",
    "soup=BeautifulSoup(resp.content)\n",
    "Text=soup.find('div',class_='_article_body_contents').text\n",
    "text=TreebankWordTokenizer().tokenize(Text)\n",
    "Pos_tag=nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    nominator=np.dot(x,y)\n",
    "    denominator=np.linalg.norm(x)*np.linalg.norm(y)\n",
    "    return nominator/denominator\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    sentence1 = [word.lower() for word in sentence1.split()]\n",
    "    sentence2 = [word.lower() for word in sentence2.split()]\n",
    "    words_ls = list(set(sentence1 + sentence2))\n",
    "    bow1 = [0] * len(words_ls)\n",
    "    bow2 = [0] * len(words_ls)\n",
    "    for word in sentence1:\n",
    "        bow1[words_ls.index(word)] += 1\n",
    "    for word in sentence2:\n",
    "        bow2[words_ls.index(word)] += 1\n",
    "    return cosine_similarity(bow1, bow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('다만 대부분 전문가는 연준이 연내 금리를 추가로 인하할 것으로 예상했다.',\n",
       " '김두언 KB증권 연구원은 \"파월 의장의 발언 이후 미국 주가가 급락하고 달러화가 상승하는 등 금융시장이 불안한 반응들을 보였다\"며 \"연준이 9월에 추가로 금리를 인하할 것\"이라고 전망했다.',\n",
       " '파월 의장의 발언을 다소 매파적으로 해석한 권희진 한화투자증권 연구원도 \"앞으로 무역 갈등이 심화해 경기에 미치는 압력이 거세지면 연준이 이에 대응해 금리를 더 내릴 가능성이 있다\"고 추가 인하 가능성은 열어놨다.')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Matrix(sentences):\n",
    "    weighted_edge=np.zeros((len(sentences), len(sentences)),dtype=np.float32)\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            weighted_edge[i][j] = sentence_similarity(sentences[i], sentences[j])\n",
    "            \n",
    "    for i in range(len(weighted_edge)):\n",
    "        weighted_edge[i] /= weighted_edge[i].sum()\n",
    "    return weighted_edge\n",
    "    \n",
    "def scoring(A,eps=0.0001,d=0.85,max_iter=50):\n",
    "    P=np.ones(len(A))/len(A)\n",
    "    for iter in range(0,max_iter):\n",
    "        newP=(1-d)+d*A.T.dot(P)\n",
    "        if abs((newP-P).sum())<=eps:\n",
    "            return newP\n",
    "        P=newP\n",
    "    return newP\n",
    "\n",
    "def summarize(text,lines=10):\n",
    "    text=sent_tokenize(text)[2:18]\n",
    "    weighted_edge=Matrix(text)\n",
    "    #print(weighted_edge)\n",
    "    score=scoring(weighted_edge)\n",
    "    rank=[item[0] for item in sorted(enumerate(score),key=lambda item:-item[1])]\n",
    "    selection=sorted(rank[:lines])\n",
    "    return itemgetter(*selection)(text)\n",
    "summarize(Text,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text=Text.split(sep=' =', maxsplit=1)[1].split(sep='    jaeh@yna.co.kr',maxsplit=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/3a/bc/1415be59292a23ff123298b4b46ec4be80b3bfe72c8d188b58ab2653dee4/gensim-3.8.0.tar.gz (23.4MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Collecting smart_open>=1.7.0 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/37/c0/25d19badc495428dec6a4bf7782de617ee0246a9211af75b302a2681dea7/smart_open-1.8.4.tar.gz (63kB)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\programdata\\anaconda3\\lib\\site-packages (from smart_open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from smart_open>=1.7.0->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart_open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/72/1a/97ca7494fd268835f2d2ea2c6b6ea3b7cfe271f22c2adb1ef45cf007d7f3/boto3-1.9.199-py2.py3-none-any.whl (128kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.7.0->gensim) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.7.0->gensim) (1.24.1)\n",
      "Collecting botocore<1.13.0,>=1.12.199 (from boto3->smart_open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/50/f8/dbe656ee191c2d8b471a86fa07f0d37515611d865deaa034fc2b71dd71e4/botocore-1.12.199-py2.py3-none-any.whl (5.6MB)\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart_open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart_open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: docutils<0.15,>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.199->boto3->smart_open>=1.7.0->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.199->boto3->smart_open>=1.7.0->gensim) (2.8.0)\n",
      "Building wheels for collected packages: gensim, smart-open\n",
      "  Building wheel for gensim (setup.py): started\n",
      "  Building wheel for gensim (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\student\\AppData\\Local\\pip\\Cache\\wheels\\2c\\19\\c6\\bf38e867cb6e75999e3ff80302eb27bdf488b333efadfbfed7\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\student\\AppData\\Local\\pip\\Cache\\wheels\\5f\\ea\\fb\\5b1a947b369724063b2617011f1540c44eb00e28c3d2ca8692\n",
      "Successfully built gensim smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.199 botocore-1.12.199 gensim-3.8.0 jmespath-0.9.4 s3transfer-0.2.1 smart-open-1.8.4\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "from gensim.summarization import keywords\n",
    "from gensim.summarization.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['한화투자증권 연구원은',\n",
       " '도널드',\n",
       " '의장의',\n",
       " '금리를 추가로 인하할 것으로',\n",
       " '이어질',\n",
       " '발언에',\n",
       " '시장의',\n",
       " '인하가',\n",
       " '요구를',\n",
       " '무역전쟁',\n",
       " '말하지',\n",
       " '입장을',\n",
       " '시작이',\n",
       " '인하를',\n",
       " '불확실성',\n",
       " '엇갈린',\n",
       " '연구원도',\n",
       " '것이라는',\n",
       " 'kb증권',\n",
       " '나왔다',\n",
       " '불안한 반응들을',\n",
       " '동참하기를 요구하고',\n",
       " '급락하고 달러화가',\n",
       " '기준금리 인하는']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords(Text).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'다만 대부분 전문가는 연준이 연내 금리를 추가로 인하할 것으로 예상했다.\\n김두언 KB증권 연구원은 \"파월 의장의 발언 이후 미국 주가가 급락하고 달러화가 상승하는 등 금융시장이 불안한 반응들을 보였다\"며 \"연준이 9월에 추가로 금리를 인하할 것\"이라고 전망했다.\\n파월 의장의 발언을 다소 매파적으로 해석한 권희진 한화투자증권 연구원도 \"앞으로 무역 갈등이 심화해 경기에 미치는 압력이 거세지면 연준이 이에 대응해 금리를 더 내릴 가능성이 있다\"고 추가 인하 가능성은 열어놨다.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [] \n",
    "wanted_POS = ['NN','NNS','NNP','NNPS', 'JJ', 'JJR', 'JJS']\n",
    "\n",
    "for word in Pos_tag:\n",
    "    if word[1] not in wanted_POS:\n",
    "        stopwords.append(word[0])\n",
    "\n",
    "punctuations = list(str(string.punctuation))\n",
    "stopwords = stopwords + punctuations\n",
    "\n",
    "stopwords_plus = ['t', 'isn']\n",
    "stopwords = stopwords + stopwords_plus \n",
    "stopwords = set(stopwords)\n",
    "\n",
    "\n",
    "processed_text = []\n",
    "for word in lemmatized_text:\n",
    "    if word not in stopwords:\n",
    "        processed_text.append(word)\n",
    "print(processed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
