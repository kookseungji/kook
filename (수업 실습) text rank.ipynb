{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"The cat sat on my face. I hate a cat.\"\n",
    "d2 = \"The dog sat on my bed. I love a dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ls=[d1,d2]\n",
    "vectorizer=TfidfVectorizer()\n",
    "tfidf=vectorizer.fit_transform(document_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a=pd.DataFrame(tfidf.todense(),columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bed</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>face</th>\n",
       "      <th>hate</th>\n",
       "      <th>love</th>\n",
       "      <th>my</th>\n",
       "      <th>on</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bed       cat       dog      face      hate      love        my  \\\n",
       "0  0.000000  0.706006  0.000000  0.353003  0.353003  0.000000  0.251164   \n",
       "1  0.353003  0.000000  0.706006  0.000000  0.000000  0.353003  0.251164   \n",
       "\n",
       "         on       sat       the  \n",
       "0  0.251164  0.251164  0.251164  \n",
       "1  0.251164  0.251164  0.251164  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.70600557, 0.        , 0.35300279, 0.35300279,\n",
       "         0.        , 0.25116439, 0.25116439, 0.25116439, 0.25116439],\n",
       "        [0.35300279, 0.        , 0.70600557, 0.        , 0.        ,\n",
       "         0.35300279, 0.25116439, 0.25116439, 0.25116439, 0.25116439]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3, 9, 8, 7, 6, 5, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vectorizer.get_feature_names())\n",
    "np.argsort(tfidf[0].toarray()).flatten()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text=Text = \"The FAANG stocks won’t see much more growth in the near future, according to Bill Studebaker, founder and Chief Investment Officer of Robo Global. \\\n",
    "Studebaker argues we are seeing a 'reallocation' that will continue from large-cap tech stocks into market-weight stocks. \\\n",
    "The FAANG stocks have had a rough few weeks, and have been hit hard since March 12. \\\n",
    "One FAANG to look out for, in the midst of all this, is Amazon, according to Studebaker. \\\n",
    "The stock market is seeing a 'reallocation' out of FAANG stocks, which are not where the smart money is, founder and Chief Investment Officer of Robo Global Bill Studebaker told Business Insider. \\\n",
    "The FAANG stocks (Facebook, Apple, Amazon, Netflix, Google) are all down considerably since March 12, a trend that accelerated when news of a massive Facebook data scandal broke, sending the tech-heavy Nasdaq into a downward frenzy. \\\n",
    "Investors are wondering what’s next. \\\n",
    "And what’s next isn’t good news for FAANG stock optimists, Studebaker thinks. 'This is a dead trade' for the next several months, he said. 'I wouldn’t expect there to be a lot of performance attribution coming from the FAANG stocks,' he added. That is, if the stock market is to see gains in the next several months, they will largely not come from the big tech companies. \\\n",
    "The market is seeing a 'reallocation out of large-cap technology, into other parts of the market,' he said. And this trend could continue for the foreseeable future. 'When you get these reallocation trades, a de-risking, this can go on for months and months.' The FAANG’s are pricey stocks, he said, pointing out that investors will 'factor in the law of big numbers,' he said. 'Just because they’re big cap doesn’t mean they’re safe,' he added. \\\n",
    "Still, he doesn’t necessarily think that investors are going to shift drastically into value stocks. 'With an increasingly favorable macro backdrop, you have strong growth demand.' \\\n",
    "Studebaker, who runs an artificial intelligence and robotics exchange-traded fund with $4 billion in assets under management, thinks that AI and robotics are better areas of growth. His ETF is up 27% in the past year, while the FAANG stocks are also largely up over that same span, even if they are down since March 12. \\\n",
    "While many point to artificial intelligence as an area that will be a boost to Google and Amazon, Studebaker doesn’t see that as a sign of significant growth for the FAANGs. He pointed out that 'eighty to ninety percent of their businesses are still search,' and that 'AI doesn’t really move the needle on the business.' He also said 'the revenue mix [attributable to AI] in those businesses are insignificant.' \\\n",
    "And while he’s not bullish on FAANG’s, he does say that the one FAANG to still watch out for is Amazon, simply because ecommerce still represents a small portion of the global retail market, giving the company room to grow.\" \n",
    "\n",
    "\n",
    "text=TreebankWordTokenizer().tokenize(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('FAANG', 'NNP'),\n",
       " ('stocks', 'NNS'),\n",
       " ('won', 'VBD'),\n",
       " ('’', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('see', 'VBP'),\n",
       " ('much', 'RB'),\n",
       " ('more', 'JJR'),\n",
       " ('growth', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('near', 'JJ'),\n",
       " ('future', 'NN'),\n",
       " (',', ','),\n",
       " ('according', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('Bill', 'NNP'),\n",
       " ('Studebaker', 'NNP'),\n",
       " (',', ','),\n",
       " ('founder', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('Chief', 'NNP'),\n",
       " ('Investment', 'NNP'),\n",
       " ('Officer', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('Robo', 'NNP'),\n",
       " ('Global.', 'NNP'),\n",
       " ('Studebaker', 'NNP'),\n",
       " ('argues', 'VBZ'),\n",
       " ('we', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('seeing', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " (\"'reallocation\", 'NN'),\n",
       " (\"'\", 'POS'),\n",
       " ('that', 'WDT'),\n",
       " ('will', 'MD'),\n",
       " ('continue', 'VB'),\n",
       " ('from', 'IN'),\n",
       " ('large-cap', 'JJ'),\n",
       " ('tech', 'NN'),\n",
       " ('stocks', 'NNS'),\n",
       " ('into', 'IN'),\n",
       " ('market-weight', 'JJ'),\n",
       " ('stocks.', 'NN'),\n",
       " ('The', 'DT'),\n",
       " ('FAANG', 'NNP'),\n",
       " ('stocks', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('had', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('rough', 'JJ'),\n",
       " ('few', 'JJ'),\n",
       " ('weeks', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('have', 'VBP'),\n",
       " ('been', 'VBN'),\n",
       " ('hit', 'VBN'),\n",
       " ('hard', 'JJ'),\n",
       " ('since', 'IN'),\n",
       " ('March', 'NNP'),\n",
       " ('12.', 'CD'),\n",
       " ('One', 'NNP'),\n",
       " ('FAANG', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('look', 'VB'),\n",
       " ('out', 'RP'),\n",
       " ('for', 'IN'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('midst', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('all', 'PDT'),\n",
       " ('this', 'DT'),\n",
       " (',', ','),\n",
       " ('is', 'VBZ'),\n",
       " ('Amazon', 'NNP'),\n",
       " (',', ','),\n",
       " ('according', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('Studebaker.', 'NNP'),\n",
       " ('The', 'DT'),\n",
       " ('stock', 'NN'),\n",
       " ('market', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('seeing', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " (\"'reallocation\", 'NN'),\n",
       " (\"'\", 'POS'),\n",
       " ('out', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('FAANG', 'NNP'),\n",
       " ('stocks', 'NNS'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('are', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('where', 'WRB'),\n",
       " ('the', 'DT'),\n",
       " ('smart', 'JJ'),\n",
       " ('money', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " (',', ','),\n",
       " ('founder', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('Chief', 'NNP'),\n",
       " ('Investment', 'NNP'),\n",
       " ('Officer', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('Robo', 'NNP'),\n",
       " ('Global', 'NNP'),\n",
       " ('Bill', 'NNP'),\n",
       " ('Studebaker', 'NNP'),\n",
       " ('told', 'VBD'),\n",
       " ('Business', 'NNP'),\n",
       " ('Insider.', 'NNP'),\n",
       " ('The', 'DT'),\n",
       " ('FAANG', 'NNP'),\n",
       " ('stocks', 'NNS'),\n",
       " ('(', '('),\n",
       " ('Facebook', 'NNP'),\n",
       " (',', ','),\n",
       " ('Apple', 'NNP'),\n",
       " (',', ','),\n",
       " ('Amazon', 'NNP'),\n",
       " (',', ','),\n",
       " ('Netflix', 'NNP'),\n",
       " (',', ','),\n",
       " ('Google', 'NNP'),\n",
       " (')', ')'),\n",
       " ('are', 'VBP'),\n",
       " ('all', 'DT'),\n",
       " ('down', 'RB'),\n",
       " ('considerably', 'RB'),\n",
       " ('since', 'IN'),\n",
       " ('March', 'NNP'),\n",
       " ('12', 'CD'),\n",
       " (',', ','),\n",
       " ('a', 'DT'),\n",
       " ('trend', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('accelerated', 'VBD'),\n",
       " ('when', 'WRB'),\n",
       " ('news', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('massive', 'JJ'),\n",
       " ('Facebook', 'NNP'),\n",
       " ('data', 'NN'),\n",
       " ('scandal', 'NN'),\n",
       " ('broke', 'VBD'),\n",
       " (',', ','),\n",
       " ('sending', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('tech-heavy', 'JJ'),\n",
       " ('Nasdaq', 'NNP'),\n",
       " ('into', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('downward', 'JJ'),\n",
       " ('frenzy.', 'NN'),\n",
       " ('Investors', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('wondering', 'VBG'),\n",
       " ('what', 'WP'),\n",
       " ('’', 'NNP'),\n",
       " ('s', 'VBD'),\n",
       " ('next.', 'DT'),\n",
       " ('And', 'CC'),\n",
       " ('what', 'WP'),\n",
       " ('’', 'NNP'),\n",
       " ('s', 'VBD'),\n",
       " ('next', 'JJ'),\n",
       " ('isn', 'NN'),\n",
       " ('’', 'NNP'),\n",
       " ('t', 'NN'),\n",
       " ('good', 'JJ'),\n",
       " ('news', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('FAANG', 'NNP'),\n",
       " ('stock', 'NN'),\n",
       " ('optimists', 'NNS'),\n",
       " (',', ','),\n",
       " ('Studebaker', 'NNP'),\n",
       " ('thinks.', 'NN'),\n",
       " (\"'This\", 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('dead', 'JJ'),\n",
       " ('trade', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('next', 'JJ'),\n",
       " ('several', 'JJ'),\n",
       " ('months', 'NNS'),\n",
       " (',', ','),\n",
       " ('he', 'PRP'),\n",
       " ('said.', 'VBD'),\n",
       " (\"'\", \"''\"),\n",
       " ('I', 'PRP'),\n",
       " ('wouldn', 'VBP'),\n",
       " ('’', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('expect', 'VBP'),\n",
       " ('there', 'EX'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('lot', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('performance', 'NN'),\n",
       " ('attribution', 'NN'),\n",
       " ('coming', 'VBG'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('FAANG', 'NNP'),\n",
       " ('stocks', 'NNS'),\n",
       " (',', ','),\n",
       " (\"'\", \"''\"),\n",
       " ('he', 'PRP'),\n",
       " ('added.', 'VBZ'),\n",
       " ('That', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " (',', ','),\n",
       " ('if', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('stock', 'NN'),\n",
       " ('market', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('see', 'VB'),\n",
       " ('gains', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('next', 'JJ'),\n",
       " ('several', 'JJ'),\n",
       " ('months', 'NNS'),\n",
       " (',', ','),\n",
       " ('they', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('largely', 'RB'),\n",
       " ('not', 'RB'),\n",
       " ('come', 'VBN'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('tech', 'NN'),\n",
       " ('companies.', 'VBZ'),\n",
       " ('The', 'DT'),\n",
       " ('market', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('seeing', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " (\"'reallocation\", 'NN'),\n",
       " ('out', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('large-cap', 'JJ'),\n",
       " ('technology', 'NN'),\n",
       " (',', ','),\n",
       " ('into', 'IN'),\n",
       " ('other', 'JJ'),\n",
       " ('parts', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('market', 'NN'),\n",
       " (',', ','),\n",
       " (\"'\", \"''\"),\n",
       " ('he', 'PRP'),\n",
       " ('said.', 'VBZ'),\n",
       " ('And', 'CC'),\n",
       " ('this', 'DT'),\n",
       " ('trend', 'NN'),\n",
       " ('could', 'MD'),\n",
       " ('continue', 'VB'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('foreseeable', 'JJ'),\n",
       " ('future.', 'NN'),\n",
       " (\"'When\", 'POS'),\n",
       " ('you', 'PRP'),\n",
       " ('get', 'VBP'),\n",
       " ('these', 'DT'),\n",
       " ('reallocation', 'NN'),\n",
       " ('trades', 'NNS'),\n",
       " (',', ','),\n",
       " ('a', 'DT'),\n",
       " ('de-risking', 'NN'),\n",
       " (',', ','),\n",
       " ('this', 'DT'),\n",
       " ('can', 'MD'),\n",
       " ('go', 'VB'),\n",
       " ('on', 'IN'),\n",
       " ('for', 'IN'),\n",
       " ('months', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('months.', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " ('The', 'DT'),\n",
       " ('FAANG', 'NNP'),\n",
       " ('’', 'NNP'),\n",
       " ('s', 'NN'),\n",
       " ('are', 'VBP'),\n",
       " ('pricey', 'JJ'),\n",
       " ('stocks', 'NNS'),\n",
       " (',', ','),\n",
       " ('he', 'PRP'),\n",
       " ('said', 'VBD'),\n",
       " (',', ','),\n",
       " ('pointing', 'VBG'),\n",
       " ('out', 'RP'),\n",
       " ('that', 'IN'),\n",
       " ('investors', 'NNS'),\n",
       " ('will', 'MD'),\n",
       " (\"'factor\", 'VB'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('law', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('big', 'JJ'),\n",
       " ('numbers', 'NNS'),\n",
       " (',', ','),\n",
       " (\"'\", \"''\"),\n",
       " ('he', 'PRP'),\n",
       " ('said.', 'VBZ'),\n",
       " (\"'Just\", 'CC'),\n",
       " ('because', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " ('’', 'VBP'),\n",
       " ('re', 'JJ'),\n",
       " ('big', 'JJ'),\n",
       " ('cap', 'NN'),\n",
       " ('doesn', 'NN'),\n",
       " ('’', 'NNP'),\n",
       " ('t', 'NN'),\n",
       " ('mean', 'NN'),\n",
       " ('they', 'PRP'),\n",
       " ('’', 'VBP'),\n",
       " ('re', 'JJ'),\n",
       " ('safe', 'JJ'),\n",
       " (',', ','),\n",
       " (\"'\", \"''\"),\n",
       " ('he', 'PRP'),\n",
       " ('added.', 'VBZ'),\n",
       " ('Still', 'RB'),\n",
       " (',', ','),\n",
       " ('he', 'PRP'),\n",
       " ('doesn', 'VBZ'),\n",
       " ('’', 'JJ'),\n",
       " ('t', 'NNS'),\n",
       " ('necessarily', 'RB'),\n",
       " ('think', 'VBP'),\n",
       " ('that', 'IN'),\n",
       " ('investors', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('shift', 'VB'),\n",
       " ('drastically', 'RB'),\n",
       " ('into', 'IN'),\n",
       " ('value', 'NN'),\n",
       " ('stocks.', 'NN'),\n",
       " (\"'With\", 'CD'),\n",
       " ('an', 'DT'),\n",
       " ('increasingly', 'RB'),\n",
       " ('favorable', 'JJ'),\n",
       " ('macro', 'NN'),\n",
       " ('backdrop', 'NN'),\n",
       " (',', ','),\n",
       " ('you', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('strong', 'JJ'),\n",
       " ('growth', 'NN'),\n",
       " ('demand.', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " ('Studebaker', 'NNP'),\n",
       " (',', ','),\n",
       " ('who', 'WP'),\n",
       " ('runs', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('artificial', 'JJ'),\n",
       " ('intelligence', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('robotics', 'NNS'),\n",
       " ('exchange-traded', 'JJ'),\n",
       " ('fund', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('$', '$'),\n",
       " ('4', 'CD'),\n",
       " ('billion', 'CD'),\n",
       " ('in', 'IN'),\n",
       " ('assets', 'NNS'),\n",
       " ('under', 'IN'),\n",
       " ('management', 'NN'),\n",
       " (',', ','),\n",
       " ('thinks', 'VBZ'),\n",
       " ('that', 'IN'),\n",
       " ('AI', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('robotics', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('better', 'JJR'),\n",
       " ('areas', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('growth.', 'NN'),\n",
       " ('His', 'PRP$'),\n",
       " ('ETF', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('up', 'RP'),\n",
       " ('27', 'CD'),\n",
       " ('%', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('past', 'JJ'),\n",
       " ('year', 'NN'),\n",
       " (',', ','),\n",
       " ('while', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('FAANG', 'NNP'),\n",
       " ('stocks', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('also', 'RB'),\n",
       " ('largely', 'RB'),\n",
       " ('up', 'IN'),\n",
       " ('over', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('same', 'JJ'),\n",
       " ('span', 'NN'),\n",
       " (',', ','),\n",
       " ('even', 'RB'),\n",
       " ('if', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('down', 'JJ'),\n",
       " ('since', 'IN'),\n",
       " ('March', 'NNP'),\n",
       " ('12.', 'CD'),\n",
       " ('While', 'IN'),\n",
       " ('many', 'JJ'),\n",
       " ('point', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('artificial', 'JJ'),\n",
       " ('intelligence', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('area', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('boost', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('Google', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Amazon', 'NNP'),\n",
       " (',', ','),\n",
       " ('Studebaker', 'NNP'),\n",
       " ('doesn', 'VBZ'),\n",
       " ('’', 'JJ'),\n",
       " ('t', 'NNS'),\n",
       " ('see', 'VBP'),\n",
       " ('that', 'IN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('sign', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('significant', 'JJ'),\n",
       " ('growth', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('FAANGs.', 'NNP'),\n",
       " ('He', 'PRP'),\n",
       " ('pointed', 'VBD'),\n",
       " ('out', 'RP'),\n",
       " ('that', 'IN'),\n",
       " (\"'eighty\", 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('ninety', 'VB'),\n",
       " ('percent', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('their', 'PRP$'),\n",
       " ('businesses', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('still', 'RB'),\n",
       " ('search', 'RB'),\n",
       " (',', ','),\n",
       " (\"'\", \"''\"),\n",
       " ('and', 'CC'),\n",
       " ('that', 'IN'),\n",
       " (\"'AI\", 'NNP'),\n",
       " ('doesn', 'VBD'),\n",
       " ('’', 'NNP'),\n",
       " ('t', 'NN'),\n",
       " ('really', 'RB'),\n",
       " ('move', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('needle', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('business.', 'NN'),\n",
       " (\"'\", 'POS'),\n",
       " ('He', 'PRP'),\n",
       " ('also', 'RB'),\n",
       " ('said', 'VBD'),\n",
       " (\"'the\", 'JJ'),\n",
       " ('revenue', 'NN'),\n",
       " ('mix', 'NN'),\n",
       " ('[', 'NNP'),\n",
       " ('attributable', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('AI', 'NNP'),\n",
       " (']', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('those', 'DT'),\n",
       " ('businesses', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('insignificant.', 'JJ'),\n",
       " (\"'\", 'POS'),\n",
       " ('And', 'CC'),\n",
       " ('while', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('’', 'VBZ'),\n",
       " ('s', 'PRP'),\n",
       " ('not', 'RB'),\n",
       " ('bullish', 'VB'),\n",
       " ('on', 'IN'),\n",
       " ('FAANG', 'NNP'),\n",
       " ('’', 'NNP'),\n",
       " ('s', 'NN'),\n",
       " (',', ','),\n",
       " ('he', 'PRP'),\n",
       " ('does', 'VBZ'),\n",
       " ('say', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('one', 'CD'),\n",
       " ('FAANG', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('still', 'RB'),\n",
       " ('watch', 'VB'),\n",
       " ('out', 'RP'),\n",
       " ('for', 'IN'),\n",
       " ('is', 'VBZ'),\n",
       " ('Amazon', 'NNP'),\n",
       " (',', ','),\n",
       " ('simply', 'RB'),\n",
       " ('because', 'IN'),\n",
       " ('ecommerce', 'NN'),\n",
       " ('still', 'RB'),\n",
       " ('represents', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('small', 'JJ'),\n",
       " ('portion', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('global', 'JJ'),\n",
       " ('retail', 'JJ'),\n",
       " ('market', 'NN'),\n",
       " (',', ','),\n",
       " ('giving', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('company', 'NN'),\n",
       " ('room', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('grow', 'VB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "Pos_tag=nltk.pos_tag(text)\n",
    "Pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "def get_word_net_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer=WordNetLemmatizer()\n",
    "lemmatized_text=[]\n",
    "for word in Pos_tag:\n",
    "    lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=get_word_net_pos(word[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'FAANG', 'stock', 'win', '’', 't', 'see', 'much', 'more', 'growth', 'in', 'the', 'near', 'future', ',', 'accord', 'to', 'Bill', 'Studebaker', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global.', 'Studebaker', 'argue', 'we', 'be', 'see', 'a', \"'reallocation\", \"'\", 'that', 'will', 'continue', 'from', 'large-cap', 'tech', 'stock', 'into', 'market-weight', 'stocks.', 'The', 'FAANG', 'stock', 'have', 'have', 'a', 'rough', 'few', 'week', ',', 'and', 'have', 'be', 'hit', 'hard', 'since', 'March', '12.', 'One', 'FAANG', 'to', 'look', 'out', 'for', ',', 'in', 'the', 'midst', 'of', 'all', 'this', ',', 'be', 'Amazon', ',', 'accord', 'to', 'Studebaker.', 'The', 'stock', 'market', 'be', 'see', 'a', \"'reallocation\", \"'\", 'out', 'of', 'FAANG', 'stock', ',', 'which', 'be', 'not', 'where', 'the', 'smart', 'money', 'be', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global', 'Bill', 'Studebaker', 'tell', 'Business', 'Insider.', 'The', 'FAANG', 'stock', '(', 'Facebook', ',', 'Apple', ',', 'Amazon', ',', 'Netflix', ',', 'Google', ')', 'be', 'all', 'down', 'considerably', 'since', 'March', '12', ',', 'a', 'trend', 'that', 'accelerate', 'when', 'news', 'of', 'a', 'massive', 'Facebook', 'data', 'scandal', 'break', ',', 'send', 'the', 'tech-heavy', 'Nasdaq', 'into', 'a', 'downward', 'frenzy.', 'Investors', 'be', 'wonder', 'what', '’', 's', 'next.', 'And', 'what', '’', 's', 'next', 'isn', '’', 't', 'good', 'news', 'for', 'FAANG', 'stock', 'optimist', ',', 'Studebaker', 'thinks.', \"'This\", 'be', 'a', 'dead', 'trade', \"'\", 'for', 'the', 'next', 'several', 'month', ',', 'he', 'said.', \"'\", 'I', 'wouldn', '’', 't', 'expect', 'there', 'to', 'be', 'a', 'lot', 'of', 'performance', 'attribution', 'come', 'from', 'the', 'FAANG', 'stock', ',', \"'\", 'he', 'added.', 'That', 'be', ',', 'if', 'the', 'stock', 'market', 'be', 'to', 'see', 'gain', 'in', 'the', 'next', 'several', 'month', ',', 'they', 'will', 'largely', 'not', 'come', 'from', 'the', 'big', 'tech', 'companies.', 'The', 'market', 'be', 'see', 'a', \"'reallocation\", 'out', 'of', 'large-cap', 'technology', ',', 'into', 'other', 'part', 'of', 'the', 'market', ',', \"'\", 'he', 'said.', 'And', 'this', 'trend', 'could', 'continue', 'for', 'the', 'foreseeable', 'future.', \"'When\", 'you', 'get', 'these', 'reallocation', 'trade', ',', 'a', 'de-risking', ',', 'this', 'can', 'go', 'on', 'for', 'month', 'and', 'months.', \"'\", 'The', 'FAANG', '’', 's', 'be', 'pricey', 'stock', ',', 'he', 'say', ',', 'point', 'out', 'that', 'investor', 'will', \"'factor\", 'in', 'the', 'law', 'of', 'big', 'number', ',', \"'\", 'he', 'said.', \"'Just\", 'because', 'they', '’', 're', 'big', 'cap', 'doesn', '’', 't', 'mean', 'they', '’', 're', 'safe', ',', \"'\", 'he', 'added.', 'Still', ',', 'he', 'doesn', '’', 't', 'necessarily', 'think', 'that', 'investor', 'be', 'go', 'to', 'shift', 'drastically', 'into', 'value', 'stocks.', \"'With\", 'an', 'increasingly', 'favorable', 'macro', 'backdrop', ',', 'you', 'have', 'strong', 'growth', 'demand.', \"'\", 'Studebaker', ',', 'who', 'run', 'an', 'artificial', 'intelligence', 'and', 'robotics', 'exchange-traded', 'fund', 'with', '$', '4', 'billion', 'in', 'asset', 'under', 'management', ',', 'think', 'that', 'AI', 'and', 'robotics', 'be', 'good', 'area', 'of', 'growth.', 'His', 'ETF', 'be', 'up', '27', '%', 'in', 'the', 'past', 'year', ',', 'while', 'the', 'FAANG', 'stock', 'be', 'also', 'largely', 'up', 'over', 'that', 'same', 'span', ',', 'even', 'if', 'they', 'be', 'down', 'since', 'March', '12.', 'While', 'many', 'point', 'to', 'artificial', 'intelligence', 'a', 'an', 'area', 'that', 'will', 'be', 'a', 'boost', 'to', 'Google', 'and', 'Amazon', ',', 'Studebaker', 'doesn', '’', 't', 'see', 'that', 'a', 'a', 'sign', 'of', 'significant', 'growth', 'for', 'the', 'FAANGs.', 'He', 'point', 'out', 'that', \"'eighty\", 'to', 'ninety', 'percent', 'of', 'their', 'business', 'be', 'still', 'search', ',', \"'\", 'and', 'that', \"'AI\", 'doesn', '’', 't', 'really', 'move', 'the', 'needle', 'on', 'the', 'business.', \"'\", 'He', 'also', 'say', \"'the\", 'revenue', 'mix', '[', 'attributable', 'to', 'AI', ']', 'in', 'those', 'business', 'be', 'insignificant.', \"'\", 'And', 'while', 'he', '’', 's', 'not', 'bullish', 'on', 'FAANG', '’', 's', ',', 'he', 'do', 'say', 'that', 'the', 'one', 'FAANG', 'to', 'still', 'watch', 'out', 'for', 'be', 'Amazon', ',', 'simply', 'because', 'ecommerce', 'still', 'represent', 'a', 'small', 'portion', 'of', 'the', 'global', 'retail', 'market', ',', 'give', 'the', 'company', 'room', 'to', 'grow', '.']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=[]\n",
    "wanted_pos=['NN','NNS','NNP','NNPS', 'JJ', 'JJR', 'JJS']\n",
    "for word in Pos_tag:\n",
    "    if word[1] not in wanted_pos:\n",
    "        stopwords.append(word[0])\n",
    "punctuations= list(str(string.punctuation))\n",
    "stopwords=stopwords+punctuations\n",
    "stopwords_plus=['t','isn']\n",
    "stopwords=stopwords+stopwords_plus\n",
    "stopwords=set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FAANG', 'stock', 'win', 'more', 'growth', 'near', 'future', 'accord', 'Bill', 'Studebaker', 'founder', 'Chief', 'Investment', 'Officer', 'Robo', 'Global.', 'Studebaker', 'argue', \"'reallocation\", 'large-cap', 'tech', 'stock', 'market-weight', 'stocks.', 'FAANG', 'stock', 'rough', 'few', 'week', 'hard', 'March', 'One', 'FAANG', 'midst', 'Amazon', 'accord', 'Studebaker.', 'stock', 'market', \"'reallocation\", 'FAANG', 'stock', 'smart', 'money', 'founder', 'Chief', 'Investment', 'Officer', 'Robo', 'Global', 'Bill', 'Studebaker', 'tell', 'Business', 'Insider.', 'FAANG', 'stock', 'Facebook', 'Apple', 'Amazon', 'Netflix', 'Google', 'March', 'trend', 'accelerate', 'news', 'massive', 'Facebook', 'data', 'scandal', 'break', 'send', 'tech-heavy', 'Nasdaq', 'downward', 'frenzy.', 'Investors', 'wonder', 'next', 'good', 'news', 'FAANG', 'stock', 'optimist', 'Studebaker', 'thinks.', \"'This\", 'dead', 'trade', 'next', 'several', 'month', 'lot', 'performance', 'attribution', 'FAANG', 'stock', 'stock', 'market', 'gain', 'next', 'several', 'month', 'big', 'tech', 'market', \"'reallocation\", 'large-cap', 'technology', 'other', 'part', 'market', 'trend', 'foreseeable', 'future.', 'reallocation', 'trade', 'de-risking', 'month', 'months.', 'FAANG', 'pricey', 'stock', 'point', 'investor', 'law', 'big', 'number', 're', 'big', 'cap', 'mean', 're', 'safe', 'investor', 'value', 'stocks.', 'favorable', 'macro', 'backdrop', 'strong', 'growth', 'demand.', 'Studebaker', 'run', 'artificial', 'intelligence', 'robotics', 'exchange-traded', 'fund', 'asset', 'management', 'AI', 'robotics', 'good', 'area', 'growth.', 'ETF', 'past', 'year', 'FAANG', 'stock', 'same', 'span', 'March', 'many', 'point', 'artificial', 'intelligence', 'area', 'boost', 'Google', 'Amazon', 'Studebaker', 'sign', 'significant', 'growth', 'FAANGs.', 'point', 'percent', 'business', \"'AI\", 'needle', 'business.', \"'the\", 'revenue', 'mix', 'attributable', 'AI', 'business', 'insignificant.', 'FAANG', 'do', 'FAANG', 'Amazon', 'ecommerce', 'represent', 'small', 'portion', 'global', 'retail', 'market', 'give', 'company', 'room']\n"
     ]
    }
   ],
   "source": [
    "processed_text = []\n",
    "for word in lemmatized_text:\n",
    "    if word not in stopwords:\n",
    "        processed_text.append(word)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=list(set(processed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'covered_coocurrences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-23885b6785d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m                     \u001b[0mindex_of_i\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow_start\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mindex_of_j\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow_start\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindex_of_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_of_j\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcovered_coocurrences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m                         \u001b[0mweighted_edge\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_of_i\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mindex_of_j\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                         \u001b[0mcovered_cooccurrences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_of_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_of_j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'covered_coocurrences' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "vocab_len=len(vocabulary)\n",
    "weighted_edge=np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "score=np.zeros((vocab_len),dtype=np.float32)\n",
    "window_size=3\n",
    "covered_concurrences=[]\n",
    "for i in range(0,vocab_len):\n",
    "    for j in range(0,vocab_len):\n",
    "        if j==i:\n",
    "            continue #같은건 skip\n",
    "        else:\n",
    "            for window_start in range(0,(len(processed_text)-window_size)):\n",
    "                window_end=window_start+window_size\n",
    "                window=processed_text[window_start:window_end]\n",
    "                if (vocabulary[i]in window) and (vocabulary[j]in window):\n",
    "                    index_of_i=window_start+window.index(vocabulary[i])\n",
    "                    index_of_j=window_start+window.index(vocabulary[j])\n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                        covered_cooccurrences.append([index_of_i,index_of_j])\n",
    "inout=np.zeros((vocab_len),dtype=np.float32)\n",
    "for i in range(0,vocab_len):\n",
    "    for j in range(0,vocab_len):\n",
    "        inout[i]+=weighted_edge[i][j]\n",
    "MAX_ITERATIONS=50\n",
    "d=0.85\n",
    "threshold=0.0001\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FAANG', 'stock', 'win'], ['more', 'growth'], ['near', 'future'], ['accord'], ['Bill', 'Studebaker'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global.', 'Studebaker', 'argue'], [\"'reallocation\"], ['large-cap', 'tech', 'stock'], ['market-weight', 'stocks.'], ['FAANG', 'stock'], ['rough', 'few', 'week'], ['hard'], ['March'], ['One', 'FAANG'], ['midst'], ['Amazon'], ['accord'], ['Studebaker.'], ['stock', 'market'], [\"'reallocation\"], ['FAANG', 'stock'], ['smart', 'money'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global', 'Bill', 'Studebaker', 'tell', 'Business', 'Insider.'], ['FAANG', 'stock'], ['Facebook'], ['Apple'], ['Amazon'], ['Netflix'], ['Google'], ['March'], ['trend'], ['accelerate'], ['news'], ['massive', 'Facebook', 'data', 'scandal', 'break'], ['send'], ['tech-heavy', 'Nasdaq'], ['downward', 'frenzy.', 'Investors'], ['wonder'], ['next'], ['good', 'news'], ['FAANG', 'stock', 'optimist'], ['Studebaker', 'thinks.', \"'This\"], ['dead', 'trade'], ['next', 'several', 'month'], ['lot'], ['performance', 'attribution'], ['FAANG', 'stock'], ['stock', 'market'], ['gain'], ['next', 'several', 'month'], ['big', 'tech'], ['market'], [\"'reallocation\"], ['large-cap', 'technology'], ['other', 'part'], ['market'], ['trend'], ['foreseeable', 'future.'], ['reallocation', 'trade'], ['de-risking'], ['month'], ['months.'], ['FAANG'], ['pricey', 'stock'], ['point'], ['investor'], ['law'], ['big', 'number'], ['re', 'big', 'cap'], ['mean'], ['re', 'safe'], ['investor'], ['value', 'stocks.'], ['favorable', 'macro', 'backdrop'], ['strong', 'growth', 'demand.'], ['Studebaker'], ['run'], ['artificial', 'intelligence'], ['robotics', 'exchange-traded', 'fund'], ['asset'], ['management'], ['AI'], ['robotics'], ['good', 'area'], ['growth.'], ['ETF'], ['past', 'year'], ['FAANG', 'stock'], ['same', 'span'], ['March'], ['many', 'point'], ['artificial', 'intelligence'], ['area'], ['boost'], ['Google'], ['Amazon'], ['Studebaker'], ['sign'], ['significant', 'growth'], ['FAANGs.'], ['point'], ['percent'], ['business'], [\"'AI\"], ['needle'], ['business.'], [\"'the\", 'revenue', 'mix'], ['attributable'], ['AI'], ['business'], ['insignificant.'], ['FAANG'], ['do'], ['FAANG'], ['Amazon'], ['ecommerce'], ['represent'], ['small', 'portion'], ['global', 'retail', 'market'], ['give'], ['company', 'room']]\n"
     ]
    }
   ],
   "source": [
    "pharses=[]\n",
    "pharse=\" \"\n",
    "for word in lemmatized_text:\n",
    "    if word in stopwords:\n",
    "        if pharse!=\" \":\n",
    "            pharses.append(str(pharse).strip().split())\n",
    "        pharse=\" \"\n",
    "    elif word not in stopwords:\n",
    "        pharse+=str(word)\n",
    "        pharse+=\" \"\n",
    "print(pharses)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phrases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-f7f85947e62d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0munique_phrases\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mphrases\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mphrase\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munique_phrases\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0munique_phrases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_phrases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'phrases' is not defined"
     ]
    }
   ],
   "source": [
    "unique_phrases=[]\n",
    "for phrase in phrases:\n",
    "    if phrase not in unique_phrases:\n",
    "        unique_phrases.append(phrase)\n",
    "print(unique_phrases)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_phrases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-ef17955c0334>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munique_phrases\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munique_phrases\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0munique_phrases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_phrases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unique_phrases' is not defined"
     ]
    }
   ],
   "source": [
    "for word in vocabulary:\n",
    "    for phrase in unique_phrases:\n",
    "        if (word in phrase) and ((word) in unique_phrases) and (len(phrase)>1):\n",
    "            unique_phrases.remove([word])\n",
    "print(unique_phrases)\n",
    "phrase_scores=[]\n",
    "keywords=[]\n",
    "for phrase in unique_phrases:\n",
    "    phrase_score=0\n",
    "    keyword=''\n",
    "    for word in phrase:\n",
    "        keyword+=str(word)\n",
    "        keyword+=\" \"\n",
    "        phrase_score+=score[vocabulary.index(word)]\n",
    "    phrase_scores.append(phrase_score)\n",
    "    keywords.append(keyword.strip())\n",
    "i=0\n",
    "for keyword in keywords:\n",
    "    print(str(keyword),str(phrase_scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
