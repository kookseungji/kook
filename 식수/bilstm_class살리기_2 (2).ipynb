{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp', 'rainfall', 'temperature', 'abnormal', 'day_sin',\n",
      "       'dayofweek_median', 'holiday', 'before_holiday', 'target_value'],\n",
      "      dtype='object')\n",
      "### merge: target TS와 related TS\n",
      "# merged_df feature 개수\n",
      "8\n",
      "\n",
      "\n",
      "# merged_df shape\n",
      "(627, 9)\n",
      "\n",
      "\n",
      "# merged_df head\n",
      "   timestamp  rainfall  temperature  abnormal   day_sin  dayofweek_median  \\\n",
      "0 2018-12-11       0.0          2.9       0.0  0.974928               187   \n",
      "1 2018-12-12       0.0          2.4       0.0  0.433884               172   \n",
      "2 2018-12-13       1.3          2.1       0.0 -0.433884               189   \n",
      "3 2018-12-14       0.0          0.6       0.0 -0.974928               151   \n",
      "4 2018-12-15       0.0          0.0       0.0 -0.781831                 0   \n",
      "\n",
      "   holiday  before_holiday  target_value  \n",
      "0        0               0         176.0  \n",
      "1        0               0         155.0  \n",
      "2        0               0         188.0  \n",
      "3        0               0         202.0  \n",
      "4        0               0           0.0  \n",
      "\n",
      "\n",
      "# merged_df tail\n",
      "     timestamp  rainfall  temperature  abnormal       day_sin  \\\n",
      "622 2020-08-23       0.0          0.0       0.0 -2.450000e-16   \n",
      "623 2020-08-24       0.0         32.0       1.0  7.818315e-01   \n",
      "624 2020-08-25       0.0         34.5       1.0  9.749279e-01   \n",
      "625 2020-08-26       0.6         34.5       1.0  4.338837e-01   \n",
      "626 2020-08-27       8.7         28.6       1.0 -4.338837e-01   \n",
      "\n",
      "     dayofweek_median  holiday  before_holiday  target_value  \n",
      "622                 0        0               0           0.0  \n",
      "623               217        0               0          79.0  \n",
      "624               187        0               0         210.0  \n",
      "625               172        0               0         164.0  \n",
      "626               189        0               0         204.0  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### hstack 생성\n",
      "# hstack[0]\n",
      "[  0.           2.9          0.           0.97492791 187.\n",
      "   0.           0.         176.        ]\n",
      "\n",
      "\n",
      "# hstack[1]\n",
      "[  0.           2.4          0.           0.43388374 172.\n",
      "   0.           0.         155.        ]\n",
      "\n",
      "\n",
      "# hstack[2]\n",
      "[  1.3          2.1          0.          -0.43388374 189.\n",
      "   0.           0.         188.        ]\n",
      "\n",
      "\n",
      "# hstack[3]\n",
      "[  0.           0.6          0.          -0.97492791 151.\n",
      "   0.           0.         202.        ]\n",
      "\n",
      "\n",
      "# hstack[-3]\n",
      "[  0.          34.5          1.           0.97492791 187.\n",
      "   0.           0.         210.        ]\n",
      "\n",
      "\n",
      "# hstack[-2]\n",
      "[  0.6         34.5          1.           0.43388374 172.\n",
      "   0.           0.         164.        ]\n",
      "\n",
      "\n",
      "# hstack[-1]\n",
      "[  8.7         28.6          1.          -0.43388374 189.\n",
      "   0.           0.         204.        ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### sequence 쪼개기\n",
      "# train X[0]\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.81831483e-01\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.45000000e-16\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.00000000e-01  5.90000000e+00  0.00000000e+00  7.81831483e-01\n",
      "   2.17000000e+02  0.00000000e+00  0.00000000e+00  2.24000000e+02]\n",
      " [ 0.00000000e+00  7.40000000e+00  0.00000000e+00  9.74927912e-01\n",
      "   1.87000000e+02  0.00000000e+00  0.00000000e+00  1.67000000e+02]\n",
      " [ 0.00000000e+00  9.50000000e+00  0.00000000e+00  4.33883739e-01\n",
      "   1.72000000e+02  0.00000000e+00  0.00000000e+00  1.34000000e+02]\n",
      " [ 0.00000000e+00  1.04000000e+01  0.00000000e+00 -4.33883739e-01\n",
      "   1.89000000e+02  0.00000000e+00  0.00000000e+00  1.70000000e+02]\n",
      " [ 0.00000000e+00  9.80000000e+00  0.00000000e+00 -9.74927912e-01\n",
      "   1.51000000e+02  0.00000000e+00  0.00000000e+00  1.31000000e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.81831483e-01\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "# train y[0]\n",
      "[  0.   0.   0. 212. 142. 110.   0.   0.]\n",
      "\n",
      "\n",
      "# train X[1]\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.45000000e-16\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.00000000e-01  5.90000000e+00  0.00000000e+00  7.81831483e-01\n",
      "   2.17000000e+02  0.00000000e+00  0.00000000e+00  2.24000000e+02]\n",
      " [ 0.00000000e+00  7.40000000e+00  0.00000000e+00  9.74927912e-01\n",
      "   1.87000000e+02  0.00000000e+00  0.00000000e+00  1.67000000e+02]\n",
      " [ 0.00000000e+00  9.50000000e+00  0.00000000e+00  4.33883739e-01\n",
      "   1.72000000e+02  0.00000000e+00  0.00000000e+00  1.34000000e+02]\n",
      " [ 0.00000000e+00  1.04000000e+01  0.00000000e+00 -4.33883739e-01\n",
      "   1.89000000e+02  0.00000000e+00  0.00000000e+00  1.70000000e+02]\n",
      " [ 0.00000000e+00  9.80000000e+00  0.00000000e+00 -9.74927912e-01\n",
      "   1.51000000e+02  0.00000000e+00  0.00000000e+00  1.31000000e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.81831483e-01\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.45000000e-16\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "# train y[1]\n",
      "[  0.   0. 212. 142. 110.   0.   0.   0.]\n",
      "\n",
      "\n",
      "# train X[-2]\n",
      "[[ 9.70000000e+01  2.73000000e+01  0.00000000e+00  9.74927912e-01\n",
      "   1.87000000e+02  0.00000000e+00  0.00000000e+00  1.67000000e+02]\n",
      " [ 0.00000000e+00  3.13000000e+01  1.00000000e+00  4.33883739e-01\n",
      "   1.72000000e+02  0.00000000e+00  0.00000000e+00  1.33000000e+02]\n",
      " [ 0.00000000e+00  3.03000000e+01  1.00000000e+00 -4.33883739e-01\n",
      "   1.89000000e+02  0.00000000e+00  0.00000000e+00  9.60000000e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -9.74927912e-01\n",
      "   1.51000000e+02  1.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.81831483e-01\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.45000000e-16\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  7.81831483e-01\n",
      "   2.17000000e+02  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  3.19000000e+01  1.00000000e+00  9.74927912e-01\n",
      "   1.87000000e+02  0.00000000e+00  0.00000000e+00  1.79000000e+02]]\n",
      "# train y[-2]\n",
      "[200. 180. 255.   0.   0.  79. 210. 164.]\n",
      "\n",
      "\n",
      "# train X[-1]\n",
      "[[ 0.00000000e+00  3.13000000e+01  1.00000000e+00  4.33883739e-01\n",
      "   1.72000000e+02  0.00000000e+00  0.00000000e+00  1.33000000e+02]\n",
      " [ 0.00000000e+00  3.03000000e+01  1.00000000e+00 -4.33883739e-01\n",
      "   1.89000000e+02  0.00000000e+00  0.00000000e+00  9.60000000e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -9.74927912e-01\n",
      "   1.51000000e+02  1.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.81831483e-01\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.45000000e-16\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  7.81831483e-01\n",
      "   2.17000000e+02  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  3.19000000e+01  1.00000000e+00  9.74927912e-01\n",
      "   1.87000000e+02  0.00000000e+00  0.00000000e+00  1.79000000e+02]\n",
      " [ 0.00000000e+00  3.14000000e+01  1.00000000e+00  4.33883739e-01\n",
      "   1.72000000e+02  0.00000000e+00  0.00000000e+00  2.00000000e+02]]\n",
      "# train y[-1]\n",
      "[180. 255.   0.   0.  79. 210. 164. 204.]\n",
      "\n",
      "\n",
      "# validation X[0]\n",
      "[[ 0.00000000e+00  2.90000000e+00  0.00000000e+00  9.74927912e-01\n",
      "   1.87000000e+02  0.00000000e+00  0.00000000e+00  1.76000000e+02]\n",
      " [ 0.00000000e+00  2.40000000e+00  0.00000000e+00  4.33883739e-01\n",
      "   1.72000000e+02  0.00000000e+00  0.00000000e+00  1.55000000e+02]\n",
      " [ 1.30000000e+00  2.10000000e+00  0.00000000e+00 -4.33883739e-01\n",
      "   1.89000000e+02  0.00000000e+00  0.00000000e+00  1.88000000e+02]\n",
      " [ 0.00000000e+00  6.00000000e-01  0.00000000e+00 -9.74927912e-01\n",
      "   1.51000000e+02  0.00000000e+00  0.00000000e+00  2.02000000e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.81831483e-01\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.45000000e-16\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.00000000e-01  5.90000000e+00  0.00000000e+00  7.81831483e-01\n",
      "   2.17000000e+02  0.00000000e+00  0.00000000e+00  2.24000000e+02]\n",
      " [ 0.00000000e+00  7.40000000e+00  0.00000000e+00  9.74927912e-01\n",
      "   1.87000000e+02  0.00000000e+00  0.00000000e+00  1.67000000e+02]]\n",
      "# validation y[0]\n",
      "[134. 170. 131.   0.   0.   0.   0. 212.]\n",
      "\n",
      "\n",
      "# validation X[1]\n",
      "[[ 0.00000000e+00  2.40000000e+00  0.00000000e+00  4.33883739e-01\n",
      "   1.72000000e+02  0.00000000e+00  0.00000000e+00  1.55000000e+02]\n",
      " [ 1.30000000e+00  2.10000000e+00  0.00000000e+00 -4.33883739e-01\n",
      "   1.89000000e+02  0.00000000e+00  0.00000000e+00  1.88000000e+02]\n",
      " [ 0.00000000e+00  6.00000000e-01  0.00000000e+00 -9.74927912e-01\n",
      "   1.51000000e+02  0.00000000e+00  0.00000000e+00  2.02000000e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.81831483e-01\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.45000000e-16\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.00000000e-01  5.90000000e+00  0.00000000e+00  7.81831483e-01\n",
      "   2.17000000e+02  0.00000000e+00  0.00000000e+00  2.24000000e+02]\n",
      " [ 0.00000000e+00  7.40000000e+00  0.00000000e+00  9.74927912e-01\n",
      "   1.87000000e+02  0.00000000e+00  0.00000000e+00  1.67000000e+02]\n",
      " [ 0.00000000e+00  9.50000000e+00  0.00000000e+00  4.33883739e-01\n",
      "   1.72000000e+02  0.00000000e+00  0.00000000e+00  1.34000000e+02]]\n",
      "# validation y[1]\n",
      "[170. 131.   0.   0.   0.   0. 212. 142.]\n",
      "\n",
      "\n",
      "# validation X[-2]\n",
      "[[ 1.30000000e+00  2.10000000e+00  0.00000000e+00 -4.33883739e-01\n",
      "   1.89000000e+02  0.00000000e+00  0.00000000e+00  1.88000000e+02]\n",
      " [ 0.00000000e+00  6.00000000e-01  0.00000000e+00 -9.74927912e-01\n",
      "   1.51000000e+02  0.00000000e+00  0.00000000e+00  2.02000000e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.81831483e-01\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.45000000e-16\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.00000000e-01  5.90000000e+00  0.00000000e+00  7.81831483e-01\n",
      "   2.17000000e+02  0.00000000e+00  0.00000000e+00  2.24000000e+02]\n",
      " [ 0.00000000e+00  7.40000000e+00  0.00000000e+00  9.74927912e-01\n",
      "   1.87000000e+02  0.00000000e+00  0.00000000e+00  1.67000000e+02]\n",
      " [ 0.00000000e+00  9.50000000e+00  0.00000000e+00  4.33883739e-01\n",
      "   1.72000000e+02  0.00000000e+00  0.00000000e+00  1.34000000e+02]\n",
      " [ 0.00000000e+00  1.04000000e+01  0.00000000e+00 -4.33883739e-01\n",
      "   1.89000000e+02  0.00000000e+00  0.00000000e+00  1.70000000e+02]]\n",
      "# validation y[-2]\n",
      "[131.   0.   0.   0.   0. 212. 142. 110.]\n",
      "\n",
      "\n",
      "# validation X[-1]\n",
      "[[ 0.00000000e+00  6.00000000e-01  0.00000000e+00 -9.74927912e-01\n",
      "   1.51000000e+02  0.00000000e+00  0.00000000e+00  2.02000000e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.81831483e-01\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.45000000e-16\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.00000000e-01  5.90000000e+00  0.00000000e+00  7.81831483e-01\n",
      "   2.17000000e+02  0.00000000e+00  0.00000000e+00  2.24000000e+02]\n",
      " [ 0.00000000e+00  7.40000000e+00  0.00000000e+00  9.74927912e-01\n",
      "   1.87000000e+02  0.00000000e+00  0.00000000e+00  1.67000000e+02]\n",
      " [ 0.00000000e+00  9.50000000e+00  0.00000000e+00  4.33883739e-01\n",
      "   1.72000000e+02  0.00000000e+00  0.00000000e+00  1.34000000e+02]\n",
      " [ 0.00000000e+00  1.04000000e+01  0.00000000e+00 -4.33883739e-01\n",
      "   1.89000000e+02  0.00000000e+00  0.00000000e+00  1.70000000e+02]\n",
      " [ 0.00000000e+00  9.80000000e+00  0.00000000e+00 -9.74927912e-01\n",
      "   1.51000000e+02  0.00000000e+00  0.00000000e+00  1.31000000e+02]]\n",
      "# validation y[-1]\n",
      "[  0.   0.   0.   0. 212. 142. 110.   0.]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 60)                9360      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1500)              91500     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               150100    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 808       \n",
      "=================================================================\n",
      "Total params: 251,768\n",
      "Trainable params: 251,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "### model 학습\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 10016.3906 - accuracy: 0.1842 - val_loss: 8139.5640 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 5457.0342 - accuracy: 0.2155 - val_loss: 9902.6113 - val_accuracy: 0.2500\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 4509.0181 - accuracy: 0.2681 - val_loss: 7541.5391 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 3942.7344 - accuracy: 0.2368 - val_loss: 8108.6162 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 3660.7056 - accuracy: 0.2599 - val_loss: 8212.6680 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 3279.9185 - accuracy: 0.3109 - val_loss: 10070.8066 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 3049.7332 - accuracy: 0.3207 - val_loss: 9902.5850 - val_accuracy: 0.2500\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 2836.9268 - accuracy: 0.3799 - val_loss: 9190.5762 - val_accuracy: 0.2500\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 2643.0139 - accuracy: 0.3865 - val_loss: 9976.3027 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 2545.7246 - accuracy: 0.4161 - val_loss: 10266.8594 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 2572.0454 - accuracy: 0.4046 - val_loss: 10212.9824 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2418.3464 - accuracy: 0.4404"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "plt.rc('font', family='NanumGothic')\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "                  'figure.figsize': (16, 8),\n",
    "                 'axes.labelsize': 'x-large',\n",
    "                 'axes.titlesize':'x-large',\n",
    "                 'xtick.labelsize':'x-large',\n",
    "                 'ytick.labelsize':'x-large',\n",
    "                 'axes.grid' : True}\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "class ldccLstm():\n",
    "    \n",
    "    ### 초기 설정\n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        ### warning 메시지 무시\n",
    "        import warnings\n",
    "        warnings.filterwarnings(action='ignore')\n",
    "        \n",
    "        \n",
    "#         ### aws python SDK 객체 생성 및 Amazon Forecast, Forecastquery, S3 에 연동, IAM 설정\n",
    "#         self.fcst = ldcc_forecast_01_3.ldccForecast()\n",
    "#         # region_name = ['서울': 'ap-northeast-2', '싱가포르': 'ap-southeast-1']\n",
    "#         self.fcst.region_name = 'ap-northeast-2'\n",
    "#         self.fcst.connect_aws()\n",
    "#         self.fcst.iam_config()\n",
    "\n",
    "        \n",
    "        ### 인자 임포트 및 선언\n",
    "        with open('bilstm_main_v1.yaml') as f:\n",
    "            self.argData = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "            # common\n",
    "            self.train_start_date = self.argData['common']['train_start_date']\n",
    "            self.train_end_date = self.argData['common']['train_end_date']\n",
    "            self.test_start_date = self.argData['common']['test_start_date']\n",
    "            self.test_end_date = self.argData['common']['test_end_date']\n",
    "            self.modelName = self.argData['common']['modelName']\n",
    "            \n",
    "#             # data_import\n",
    "#             self.train_readS3BucketName = self.argData['data_import']['train_readS3BucketName']\n",
    "#             self.train_readS3Key = self.argData['data_import']['train_readS3Key']  \n",
    "#             self.rel_readS3BucketName = self.argData['data_import']['rel_readS3BucketName']\n",
    "#             self.rel_readS3Key = self.argData['data_import']['rel_readS3Key']\n",
    "            \n",
    "            # data_prep\n",
    "            self.n_steps_in = self.argData['data_prep']['n_steps_in']\n",
    "            self.n_steps_out = self.argData['data_prep']['n_steps_out']\n",
    "#            self.features = self.argData['data_prep']['features']\n",
    "\n",
    "            # train_model\n",
    "            self.cell = self.argData['train_model']['cell']\n",
    "            self.epochs = self.argData['train_model']['epochs']\n",
    "            self.batch_size = self.argData['train_model']['batch_size']\n",
    "            self.verbose = self.argData['train_model']['verbose']\n",
    "            \n",
    "            # save_res\n",
    "            self.upload_to_s3_bool = self.argData['save_res']['upload_to_s3_bool']\n",
    "            self.save_dir = self.argData['save_res']['save_dir']\n",
    "        \n",
    "    \n",
    "    ### local에서 데이터 가져오기    \n",
    "    def data_import(self, data_location, columns_list):\n",
    "        \n",
    "        self.merged_df = pd.read_csv('input2_daysin.csv')\n",
    "        self.merged_df = pd.read_csv(data_location)\n",
    "        self.merged_df = self.merged_df.astype({'timestamp':'datetime64[ns]'})\n",
    "        #self.merged_df = self.merged_df[['timestamp', 'rainfall', 'temperature', 'abnormal', 'weekday', 'dayofweek_median', 'holiday', 'before_holiday', 'target_value']]\n",
    "        self.merged_df = self.merged_df[columns_list]\n",
    "        \n",
    "        ## local에서 train 데이터 만들기\n",
    "        \n",
    "        self.target_df = self.merged_df[\n",
    "            (self.merged_df['timestamp'] >= self.test_start_date)\n",
    "            &\n",
    "            (self.merged_df['timestamp'] <= self.test_end_date)\n",
    "        ]\n",
    "    \n",
    " \n",
    "        \n",
    "        \n",
    "        \n",
    "#         ## target TS 임포트\n",
    "#         self.fcst.readS3BucketName = self.train_readS3BucketName \n",
    "#         self.fcst.readS3Key = self.train_readS3Key\n",
    "#         self.target_df = self.fcst.read_s3data()   \n",
    "#         self.target_df = self.target_df[['timestamp', 'target_value', 'item_id']]\n",
    "#         print('### target_df 생성')\n",
    "#         print('# target_df columnlist')\n",
    "#         print(self.target_df.columns)\n",
    "#         print('\\n')\n",
    "#         print('# target_df head')\n",
    "#         print(self.target_df.head())\n",
    "#         print('\\n')\n",
    "#         print('# target_df tail')\n",
    "#         print(self.target_df.tail())\n",
    "#         print('\\n\\n\\n')\n",
    "        \n",
    "        \n",
    "#         ## related TS 임포트\n",
    "#         self.fcst.readS3BucketName = self.rel_readS3BucketName \n",
    "#         self.fcst.readS3Key = self.rel_readS3Key\n",
    "#         self.rel_df = self.fcst.read_s3data()        \n",
    "#         self.rel_df = self.rel_df[self.features]\n",
    "#         print('### related TS 피처 선택')\n",
    "#         print('# rel_df columnlist')\n",
    "#         print(self.rel_df.columns)\n",
    "#         print('\\n')\n",
    "#         print('# rel_df head')\n",
    "#         print(self.rel_df.head())\n",
    "#         print('\\n')\n",
    "#         print('# rel_df tail')\n",
    "#         print(self.rel_df.tail())\n",
    "#         print('\\n\\n\\n')\n",
    "        \n",
    "#         ## merge: target TS와 related TS\n",
    "#         self.merged_df = pd.merge(self.target_df, self.rel_df, \n",
    "#                                   on=['timestamp', 'item_id'], how='inner')\n",
    "\n",
    "        \n",
    "        ## train 데이터 학습 날짜 선정\n",
    "        self.merged_df = self.merged_df[\n",
    "            (self.merged_df['timestamp'] >= self.train_start_date) \n",
    "            & \n",
    "            (self.merged_df['timestamp'] <= self.train_end_date)\n",
    "        ]\n",
    "        \n",
    "        print(self.merged_df.columns)\n",
    "        \n",
    "\n",
    "        ## target_value 칼럼 맨 뒤로 빼기\n",
    "        col_list = []\n",
    "        for i in self.merged_df.columns:\n",
    "            col_list.append(i)\n",
    "        col_list.remove('target_value')\n",
    "        col_list.append('target_value')\n",
    "\n",
    "        self.merged_df = self.merged_df[col_list]\n",
    "        \n",
    "        ## n_features: 피처 개수 선언\n",
    "        self.n_features = len(self.merged_df.columns) - 1        \n",
    "        \n",
    "        print('### merge: target TS와 related TS')\n",
    "        print('# merged_df feature 개수')\n",
    "        print(self.n_features)\n",
    "        print('\\n')        \n",
    "        print('# merged_df shape')\n",
    "        print(self.merged_df.shape)\n",
    "        print('\\n')\n",
    "        print('# merged_df head')\n",
    "        print(self.merged_df.head())\n",
    "        print('\\n')\n",
    "        print('# merged_df tail')\n",
    "        print(self.merged_df.tail())\n",
    "        print('\\n\\n\\n')\n",
    "        \n",
    "                \n",
    "    \n",
    "    \n",
    "    ### 데이터 전처리\n",
    "    def data_prep(self):\n",
    "        \n",
    "        ## hstack 생성\n",
    "        # 피처별 어레이 데이터를 담을 딕셔너리 선언\n",
    "        feat_dict = {}\n",
    "        \n",
    "        # 피처가 아닌 칼럼 제거\n",
    "        column_list = self.merged_df.columns.drop(['timestamp'])\n",
    "        \n",
    "        # 각 피처 별로 어레이 타입으로 딕셔너리 구성 후, hstack으로 합치기 \n",
    "        for column_name in column_list:\n",
    "            feat_dict['%s' %column_name] = array(\n",
    "                self.merged_df['%s' %column_name]).reshape(\n",
    "                (len(self.merged_df['%s' %column_name]),1))\n",
    "        \n",
    "        self.raw_dataset = hstack(feat_dict.values())\n",
    "        \n",
    "        print('### hstack 생성')\n",
    "        for i in (0, 1, 2, 3, -3, -2, -1):\n",
    "            print('# hstack[%s]' %i)\n",
    "            print(self.raw_dataset[i])\n",
    "            print('\\n')        \n",
    "        print('\\n\\n\\n')\n",
    "    \n",
    "    \n",
    "        ## 시퀀스 쪼개기\n",
    "        # X, y로 쪼개기 위한 리스트 선언\n",
    "        X, y = [], []\n",
    "\n",
    "        # 시퀀스 길이만큼 1 step으로 수행\n",
    "        for i in range(len(self.raw_dataset)):\n",
    "            # X의 미만 위치, y의 이상 위치\n",
    "            end_ix = i + self.n_steps_in\n",
    "            # y의 미만 위치\n",
    "            out_end_ix = end_ix + self.n_steps_out\n",
    "            # y의 미만 위치가 시퀀스 길이를 넘으면 종료\n",
    "            if out_end_ix > len(self.raw_dataset):\n",
    "                break\n",
    "            # 시퀀스에서 multistep으로 X, y를 쪼개기\n",
    "            x_seq, y_seq = self.raw_dataset[i:end_ix], self.raw_dataset[end_ix:out_end_ix, -1]\n",
    "            X.append(x_seq)\n",
    "            y.append(y_seq)\n",
    "        \n",
    "#         self.X = array(X).astype(np.int64)\n",
    "#         self.y = array(y).astype(np.int64)\n",
    "        self.X = array(X)\n",
    "        self.y = array(y)\n",
    "        \n",
    "        self.train_X = self.X[4:]\n",
    "        self.train_y = self.y[4:]\n",
    "        self.val_X = self.X[:4]\n",
    "        self.val_y = self.y[:4]\n",
    "        \n",
    "        print('### sequence 쪼개기')\n",
    "        for i in (0, 1, -2, -1):\n",
    "            print('# train X[%s]' %i)\n",
    "            print(self.train_X[i])\n",
    "            print('# train y[%s]' %i)\n",
    "            print(self.train_y[i])            \n",
    "            print('\\n')        \n",
    "            \n",
    "        for i in (0, 1, -2, -1):\n",
    "            print('# validation X[%s]' %i)\n",
    "            print(self.val_X[i])\n",
    "            print('# validation y[%s]' %i)\n",
    "            print(self.val_y[i])            \n",
    "            print('\\n')              \n",
    "        print('\\n\\n\\n')\n",
    "    \n",
    "    \n",
    "    ### 모델 학습\n",
    "    def train_model(self):        \n",
    "        # 모델 정의                \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Bidirectional(LSTM(self.cell, activation='relu'), input_shape=(self.n_steps_in, self.n_features)))\n",
    "        \n",
    "        self.model.add(Dense(1500))\n",
    "        self.model.add(Dense(100))\n",
    "        self.model.add(Dense(self.n_steps_out))\n",
    "        \n",
    "        self.model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "        self.model.summary()\n",
    "        print('\\n\\n')\n",
    "\n",
    "        # 모델 fit\n",
    "        print('### model 학습')\n",
    "        hist = self.model.fit(self.train_X, self.train_y, \n",
    "                              validation_data=(self.val_X, self.val_y), \n",
    "                              epochs=self.epochs, \n",
    "                              batch_size = self.batch_size,\n",
    "                              verbose = self.verbose)\n",
    "        \n",
    "\n",
    "        fig, loss_ax = plt.subplots()\n",
    "\n",
    "        acc_ax = loss_ax.twinx()\n",
    "\n",
    "        loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "        loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "        acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "        acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')        \n",
    "        \n",
    "        loss_ax.set_xlabel('epoch')\n",
    "        loss_ax.set_ylabel('loss')\n",
    "        acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "        print('\\n\\n\\n')\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "    ### 모델 검증\n",
    "    def run_pred(self):\n",
    "        self.test_start_date = datetime.datetime.strptime(self.test_start_date, '%Y-%m-%d')           \n",
    "        self.test_end_date = datetime.datetime.strptime(self.test_end_date, '%Y-%m-%d')         \n",
    "        ## input_df 선언\n",
    "        self.input_start_date = self.test_start_date + datetime.timedelta(days=-(self.n_steps_in+3))\n",
    "        print('input_start_date: ', self.input_start_date)\n",
    "        self.input_end_date = self.test_start_date + datetime.timedelta(days=-1)\n",
    "\n",
    "        self.input_df = self.merged_df[\n",
    "            (self.merged_df['timestamp'] >= self.input_start_date) \n",
    "            &\n",
    "            (self.merged_df['timestamp'] <= self.input_end_date)\n",
    "        ]\n",
    "        print('### input_df 선언')        \n",
    "        print('# input_df shape')\n",
    "        print(self.input_df.shape)\n",
    "        print()\n",
    "        print('# input_df head')\n",
    "        print(self.input_df.head())\n",
    "        print()\n",
    "        print('# input_df tail')\n",
    "        print(self.input_df.tail())\n",
    "        print('\\n\\n\\n')\n",
    "        \n",
    "        ## input_x_seq 선언\n",
    "        # hstack 생성\n",
    "        # 피처별 어레이 데이터를 담을 딕셔너리 선언\n",
    "        feat_dict = {}\n",
    "        # 피처가 아닌 칼럼 제거\n",
    "        column_list = self.input_df.columns.drop(['timestamp'])\n",
    "        # 각 피처 별로 어레이 타입으로 딕셔너리 구성 후, hstack으로 합치기 \n",
    "        for column_name in column_list:\n",
    "            feat_dict['%s' %column_name] = array(\n",
    "                self.input_df['%s' %column_name]).reshape(\n",
    "                (len(self.input_df['%s' %column_name]),1))\n",
    "        self.input_raw_dataset = hstack(feat_dict.values())\n",
    "        print('### hstack 생성')\n",
    "        print('# hstack[0]')\n",
    "        print(self.input_raw_dataset[0])\n",
    "        print('\\n\\n\\n')\n",
    "        # 시퀀스로 생성        \n",
    "#         self.input_x_seq = array(self.input_raw_dataset).astype(np.int64)\n",
    "#         self.input_x_seq = self.input_x_seq.reshape(1,self.input_x_seq.shape[0],self.input_x_seq.shape[1])\n",
    "        self.input_x_seq = array(self.input_raw_dataset)\n",
    "        self.input_x_seq = self.input_x_seq.reshape(1,self.input_x_seq.shape[0],self.input_x_seq.shape[1])\n",
    "        print('### input_x 선언')\n",
    "        print('# input_x_seq shape')\n",
    "        print(self.input_x_seq.shape)\n",
    "        print('\\n') \n",
    "        print('# input_x_seq')\n",
    "        print(self.input_x_seq)\n",
    "        print('\\n\\n\\n')\n",
    "        \n",
    "        \n",
    "        ## 예측 수행\n",
    "        self.yhat = self.model.predict(self.input_x_seq, verbose=0).astype(np.int64)      \n",
    "        print('### 예측 수행')\n",
    "        print('# yhat')\n",
    "        print(self.yhat)\n",
    "        print('\\n\\n\\n')\n",
    "        \n",
    "        \n",
    "        ## rst_df 생성\n",
    "        # rst_df 1일 간격 날짜 변수 선언\n",
    "        rst_date_var = self.test_start_date\n",
    "        # 날짜를 입력할 리스트\n",
    "        rst_date_list = []\n",
    "        \n",
    "        while rst_date_var != self.test_end_date + datetime.timedelta(days=1):\n",
    "            rst_date_list.append(rst_date_var)\n",
    "            rst_date_var += datetime.timedelta(days=1)\n",
    "        \n",
    "        self.rst_df = pd.DataFrame(rst_date_list, columns=['timestamp'])\n",
    "        \n",
    "        ## rst_df에 yhat 칼럼 추가\n",
    "        self.rst_df['yhat'] = self.yhat[0][3:]\n",
    "        print('### rst_df 생성')\n",
    "        print('# rst_df')\n",
    "        print(self.rst_df)        \n",
    "        print('\\n\\n\\n')\n",
    "        \n",
    "        ## y가 있다면 rst_df에 label 칼럼 선언               \n",
    "        if self.target_df.iloc[-1,:]['timestamp'] > self.test_end_date:\n",
    "            self.label = self.target_df[\n",
    "                (self.target_df['timestamp'] >= self.test_start_date) \n",
    "                &\n",
    "                (self.target_df['timestamp'] <= self.test_end_date)\n",
    "            ][['target_value']]\n",
    "            \n",
    "            self.rst_df['label'] = list(self.label.target_value)\n",
    "            \n",
    "            print('### rst_df에 label 칼럼 추가')        \n",
    "            print('# label')\n",
    "            print(self.label)\n",
    "            print('\\n')\n",
    "            print('# rst_df')\n",
    "            print(self.rst_df)\n",
    "            print('\\n\\n\\n')            \n",
    "        else:\n",
    "            print('### label_df 선언')        \n",
    "            print('- 예측하고자 하는 기간의 실제값이 없으므로 label_df 선언 X')\n",
    "            print('# rst_df')\n",
    "            print(self.rst_df)            \n",
    "            print('\\n\\n\\n')\n",
    "    \n",
    "    \n",
    "    ### 시각화\n",
    "    def visualize_rst(self):\n",
    "        \n",
    "        plt.subplots()\n",
    "\n",
    "        plt.title('예측 결과', fontsize = 20)\n",
    "\n",
    "        plt.plot(self.rst_df['timestamp'], self.rst_df['yhat'], label= '예측', linewidth = 2, linestyle = '--')\n",
    "\n",
    "        plt.plot(self.target_df['timestamp'], self.target_df['target_value'], label = '실제', linewidth = 2)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('Vol')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#     ### 라인그래프 시각화\n",
    "#     def visualize_rst(self):\n",
    "#         figure, ((ax1)) = plt.subplots(nrows=1, ncols=1)\n",
    "#         figure.set_size_inches(18,8)\n",
    "\n",
    "#         #legend list 선언\n",
    "#         legend_list = []    \n",
    "\n",
    "#         col_list = list(self.rst_df.columns)\n",
    "#         col_list.remove('timestamp')\n",
    "#         for line in col_list:\n",
    "#             sns.lineplot(x='timestamp', \n",
    "#                          y='%s' %line, \n",
    "#                          data=self.rst_df, \n",
    "#                          linewidth=4,\n",
    "#                          label='big',\n",
    "#                          ax=ax1\n",
    "#                         )\n",
    "\n",
    "#         # legend list 붙이기\n",
    "#         legend_list.append('lunch' + ' ' + line)\n",
    "\n",
    "#         # legend 생성\n",
    "#         plt.legend(title='Line', loc='upper right', \n",
    "#                    fontsize='x-large', title_fontsize='50', \n",
    "#                    labels=legend_list)                        \n",
    "\n",
    "#         # x, y limit 설정    \n",
    "#         xlim = [self.rst_df['timestamp'][0], self.rst_df['timestamp'][-1:]]\n",
    "#         ax1.set_xlim(xlim)    \n",
    "#         ylim = [0,600]\n",
    "#         ax1.set_ylim(ylim)\n",
    "\n",
    "#         # x, y 라벨 이름, 크기\n",
    "#         ax1.set_xlabel('Date',fontsize=40);\n",
    "#         ax1.set_ylabel('Count',fontsize=40);                \n",
    "\n",
    "#         # major tick 폰트 사이즈 설정\n",
    "#         ax1.tick_params(axis='both', which='major', labelsize=30)\n",
    "\n",
    "\n",
    "#         # figure를 변수로 선언\n",
    "#         self.figure = figure\n",
    "        \n",
    "       \n",
    "    ### error metrics 계산\n",
    "    def error_metric(self):\n",
    "        self.y_pred = self.rst_df['yhat'].tolist()\n",
    "        self.y_true = self.target_df['target_value'].tolist()\n",
    "        self.rmse = mean_squared_error(self.y_pred, self.y_true)**0.5\n",
    "        \n",
    "\n",
    "\n",
    "#     ### error metrics 계산        \n",
    "#     def error_metric(self):\n",
    "        \n",
    "#         # y가 있다면\n",
    "#         if self.target_df.iloc[-1,:]['timestamp'] > self.test_end_date:\n",
    "\n",
    "#             ## RMSE\n",
    "#             label = self.rst_df['label']\n",
    "#             yhat = self.rst_df['yhat']\n",
    "#             self.rmse = mean_squared_error(label, yhat)**0.5\n",
    "#             print('### RMSE')\n",
    "#             print(self.rmse)\n",
    "#             print('\\n\\n')\n",
    "\n",
    "#             ## MAPE\n",
    "#             # 타우가 0.5일 때\n",
    "#             # 분자 리스트 선언\n",
    "#             numer_list = []\n",
    "#             for i in range(0, len(list(self.rst_df))):\n",
    "#                 # 실제값 - 예측값 \n",
    "#                 abs_sub_val = abs(list(yhat)[i] - list(label)[i])                \n",
    "            \n",
    "#                 numer_list.append(abs_sub_val)\n",
    "            \n",
    "#             total_numerator = sum(numer_list)\n",
    "#             total_denominator = sum(list(label))\n",
    "            \n",
    "#             self.mape = 2 * (total_numerator / total_denominator)\n",
    "            \n",
    "#             print('### MAPE by AWS')\n",
    "#             print(self.mape)\n",
    "#             print('\\n\\n')\n",
    "            \n",
    "#             ## error_metric_df\n",
    "#             error_list = []\n",
    "#             error_list.append(self.modelName)\n",
    "#             error_list.append(self.rmse)\n",
    "#             error_list.append(self.mape)\n",
    "#             error_list\n",
    "            \n",
    "#             self.error_df = pd.DataFrame([error_list], \n",
    "#                                     columns=['model_name',\n",
    "#                                             'rmse',\n",
    "#                                             'mape'])\n",
    "            \n",
    "#             print('### error_df 생성')\n",
    "#             print(self.error_df)\n",
    "#             print('\\n\\n\\n')\n",
    "\n",
    "#         else:\n",
    "#             print('예측하고자 하는 구간의 실제값이 없으므로 error_metric 생략')\n",
    "#             print('\\n\\n\\n')\n",
    "#             pass\n",
    "        \n",
    "#         return True\n",
    "        \n",
    "    \n",
    "\n",
    "    ### 결과 저장\n",
    "    def save_rst(self):\n",
    "        ## 로컬에 저장\n",
    "        # system_rst 불러오기\n",
    "        self.sys_rst_df = pd.read_csv('system_rst.csv') \n",
    "        # sys_rst_df에 맞게 rst_df 수정\n",
    "        self.ref_rst_df = self.rst_df[['timestamp', 'yhat']]\n",
    "        self.ref_rst_df['running_time'] = datetime.datetime.now()\n",
    "        self.sys_rst_df = self.sys_rst_df.append(self.ref_rst_df)\n",
    "\n",
    "        # sys_rst_df 저장\n",
    "        self.sys_rst_df.to_csv('system_rst.csv', index=False)\n",
    "\n",
    "        print('### system_rst.csv 업데이트 후 local에 저장')\n",
    "        print('# system_rst.csv')\n",
    "        print(self.sys_rst_df)\n",
    "        print('\\n\\n\\n')\n",
    "        \n",
    "        \n",
    "#         ## s3에 저장\n",
    "#         # system_rst.csv가 있다면 제거\n",
    "#         self.objects_list_df = pd.DataFrame(\n",
    "#             boto3.client('s3',\n",
    "#                 aws_access_key_id=self.fcst.aws_access_key_id,\n",
    "#                 aws_secret_access_key=self.fcst.aws_secret_access_key).list_objects(\n",
    "#                 Bucket=self.train_readS3BucketName)['Contents']\n",
    "#         )\n",
    "#         print(self.objects_list_df)\n",
    "        \n",
    "#         if 'result/system_rst.csv' in list(self.objects_list_df.Key):\n",
    "\n",
    "            \n",
    "#             self.fcst.s3.delete_object(\n",
    "#                 Bucket='ldcc-cafeteria-01-bucket-bellship',\n",
    "#                 Key='result/system_rst.csv') \n",
    "#             print('### 기존 system_rst.csv 삭제')\n",
    "#             print('\\n\\n\\n')\n",
    "#         else:\n",
    "#             print('### system_rst.csv 존재 X')\n",
    "\n",
    "#         self.fcst.toS3BucketName = 'ldcc-cafeteria-01-bucket-bellship'\n",
    "#         self.fcst.toS3Key = 'result/system_rst.csv'\n",
    "#         self.fcst.toS3LocalDataName = 'system_rst.csv'\n",
    "#         self.fcst.upload_local_to_s3()        \n",
    "        \n",
    "#        return True\n",
    "    \n",
    "    \n",
    "    def show_param(self):\n",
    "        print(' - train start: ', self.argData['common']['train_start_date'])\n",
    "        print(' - train end: ', self.argData['common']['train_end_date'])\n",
    "        print(' - test start: ', self.argData['common']['test_start_date'])\n",
    "        print(' - test end: ', self.argData['common']['test_end_date'])        \n",
    "        print(' - merged_df column name: ')\n",
    "        print(self.merged_df.columns)        \n",
    "        print(' - input의 스탭 수', self.n_steps_in)\n",
    "        print(' - output의 스탭 수', self.n_steps_out)\n",
    "        print(' - 피처 수: ', self.n_features)\n",
    "        print(' - 에포크: ', self.argData['train_model']['epochs'])\n",
    "        print(' - 모델명: ', self.argData['common']['modelName'])\n",
    "        print('\\n')\n",
    "        print(' - 실제 값: ', self.y_true)\n",
    "        print(' - 예측 값: ', self.y_pred)\n",
    "        print('================================================')\n",
    "        print('rmse : ', self.rmse)\n",
    "            \n",
    "    \n",
    "#################################################################    \n",
    "\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ### 0. convlstm 객체 생성\n",
    "    ldcclstm = ldccLstm()    \n",
    "    \n",
    "    ### 1. 데이터 임포트\n",
    "        ### 데이터 주소 입력\n",
    "        ### column list 입력\n",
    "    lst = ['timestamp', 'rainfall', 'temperature', 'abnormal', 'day_sin', 'dayofweek_median', 'holiday', 'before_holiday', 'target_value']\n",
    "    ldcclstm.data_import('input2_daysin.csv', lst)\n",
    "\n",
    "    ### 2. 데이터 전처리\n",
    "    ldcclstm.data_prep()\n",
    "    \n",
    "    ### 3. 모델 학습\n",
    "    ldcclstm.train_model()\n",
    "    \n",
    "    ### 4. 예측 수행\n",
    "    ldcclstm.run_pred()\n",
    "    \n",
    "    ### 5. 결과 시각화\n",
    "    ldcclstm.visualize_rst()\n",
    "\n",
    "    ### 6. 에러 메트릭 생성\n",
    "    ldcclstm.error_metric()\n",
    "    \n",
    "    ### 7. 결과 저장\n",
    "#    ldcclstm.save_rst()\n",
    "    \n",
    "    ### 0. 파라미터 출력\n",
    "    ldcclstm.show_param()\n",
    "    \n",
    "    print('### Completed')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.3 on Python 3.6 (CUDA 10.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
