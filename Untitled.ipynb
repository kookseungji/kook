{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "포도 0.7806870451028084\n",
      "사과 0.4418867148991048\n",
      "바나나 0.4418867148991047\n",
      "topic 1\n",
      "스시 0.8863983395567693\n",
      "라면 0.33189108645535886\n",
      "소바 0.22819541007925928\n",
      "topic 2\n",
      "짬뽕 0.6258000769372029\n",
      "짜장면 0.6258000769372029\n",
      "탕수욕 0.43613964825400603\n"
     ]
    }
   ],
   "source": [
    "doc_ls = ['바나나 사과 포도 포도',\n",
    "'사과 포도',\n",
    "'포도 바나나',\n",
    "'짜장면 짬뽕 탕수욕',\n",
    "'볶음밥 탕수욕',\n",
    "'짜장면 짬뽕',\n",
    "'라면 스시',\n",
    "'스시',\n",
    "'가츠동 스시 소바',\n",
    "'된장찌개 김치찌개 김치',\n",
    "'김치 된장',\n",
    "'비빔밥 김치'\n",
    "]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(max_features=1000,max_df=0.5, smooth_idf=True)\n",
    "x=vectorizer.fit_transform(doc_ls)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_model=TruncatedSVD(n_components=3,algorithm='randomized',n_iter=100)\n",
    "svd_model.fit(x)\n",
    "\n",
    "import numpy as np\n",
    "np.shape(svd_model.components_)\n",
    "terms=vectorizer.get_feature_names()\n",
    "def get(components,feature_names,n=3):\n",
    "    for idx,topic in enumerate(components):\n",
    "        print('topic',idx)\n",
    "        for i in topic.argsort()[:-n-1:-1]:\n",
    "            print(feature_names[i],topic[i])\n",
    "get(svd_model.components_,terms)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "news_df=pd.DataFrame({'document':documents})\n",
    "news_df['clean_document']=news_df['document'].str.replace('[^a-zA-Z]',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_df['clean_document']=news_df['clean_document'].apply(lambda x: ' '.join([i for i in x.split() if len(i)>3 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_df['clean_document']=news_df['clean_document'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.023*\"game\" + 0.021*\"team\" + 0.016*\"games\" + 0.015*\"play\"')\n",
      "(1, '0.071*\"entry\" + 0.028*\"rules\" + 0.028*\"section\" + 0.026*\"build\"')\n",
      "(2, '0.047*\"char\" + 0.011*\"jagr\" + 0.010*\"regulated\" + 0.010*\"symbol\"')\n",
      "(3, '0.017*\"bike\" + 0.013*\"engine\" + 0.012*\"cars\" + 0.010*\"water\"')\n",
      "(4, '0.011*\"drive\" + 0.009*\"would\" + 0.009*\"like\" + 0.008*\"card\"')\n",
      "(5, '0.010*\"would\" + 0.008*\"right\" + 0.005*\"time\" + 0.005*\"even\"')\n",
      "(6, '0.013*\"jesus\" + 0.009*\"israel\" + 0.009*\"jews\" + 0.008*\"christian\"')\n",
      "(7, '0.017*\"would\" + 0.012*\"people\" + 0.011*\"think\" + 0.010*\"know\"')\n",
      "(8, '0.014*\"encryption\" + 0.013*\"government\" + 0.012*\"chip\" + 0.012*\"security\"')\n",
      "(9, '0.023*\"pain\" + 0.020*\"doctor\" + 0.016*\"patients\" + 0.016*\"disease\"')\n",
      "(10, '0.045*\"space\" + 0.017*\"nasa\" + 0.010*\"launch\" + 0.008*\"earth\"')\n",
      "(11, '0.011*\"motorcycle\" + 0.010*\"marriage\" + 0.009*\"pitcher\" + 0.009*\"mask\"')\n",
      "(12, '0.016*\"pens\" + 0.014*\"caps\" + 0.014*\"jets\" + 0.013*\"stanley\"')\n",
      "(13, '0.013*\"cover\" + 0.010*\"playoff\" + 0.010*\"copies\" + 0.008*\"germany\"')\n",
      "(14, '0.015*\"vesa\" + 0.012*\"rockefeller\" + 0.010*\"cross\" + 0.010*\"width\"')\n",
      "(15, '0.030*\"ground\" + 0.027*\"wire\" + 0.016*\"wiring\" + 0.015*\"panel\"')\n",
      "(16, '0.011*\"university\" + 0.011*\"information\" + 0.009*\"mail\" + 0.009*\"computer\"')\n",
      "(17, '0.020*\"armenian\" + 0.019*\"said\" + 0.018*\"armenians\" + 0.013*\"people\"')\n",
      "(18, '0.017*\"file\" + 0.011*\"program\" + 0.010*\"windows\" + 0.008*\"files\"')\n",
      "(19, '0.007*\"president\" + 0.007*\"government\" + 0.006*\"health\" + 0.006*\"years\"')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords=stopwords.words('english')\n",
    "tokenized_doc=news_df['clean_document'].apply(lambda x: x.split()).apply(lambda x : [item for item in x if item not in stopwords])\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary=corpora.Dictionary(tokenized_doc)\n",
    "corpus=[dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "\n",
    "import gensim \n",
    "ldamodel=gensim.models.ldamodel.LdaModel(corpus,num_topics=20,id2word=dictionary,passes=15)\n",
    "topics=ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
