{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (3.4.0.3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/?ss=ai-big-data#45dd5dd61f25'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "eng_news = soup.select('p') #[class=\"speakable-paragraph\"]')\n",
    "eng_text = eng_news[3].get_text()\n",
    "\n",
    "eng_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barak', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "token1=word_tokenize('Barak Obama likes fried chicken very much')\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "token1=word_tokenize(eng_text)\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2= WordPunctTokenizer().tokenize(eng_text) #모든 구두점으로 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'\", 's', 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i', '.', 'e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "print(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer #스페이스와 구두점으로 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e.', 'every', 'job.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "token3=TreebankWordTokenizer().tokenize(eng_text)\n",
    "print(token3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('And', 'CC'), ('yes', 'UH'), (',', ','), ('she', 'PRP'), ('does', 'VBZ'), ('mean', 'VB'), ('everybody', 'NN'), (\"'s\", 'POS'), ('job', 'NN'), ('from', 'IN'), ('yours', 'NNS'), ('to', 'TO'), ('mine', 'VB'), ('and', 'CC'), ('onward', 'VB'), ('to', 'TO'), ('the', 'DT'), ('role', 'NN'), ('of', 'IN'), ('grain', 'NN'), ('farmers', 'NNS'), ('in', 'IN'), ('Egypt', 'NNP'), (',', ','), ('pastry', 'NN'), ('chefs', 'NNS'), ('in', 'IN'), ('Paris', 'NNP'), ('and', 'CC'), ('dog', 'NN'), ('walkers', 'NNS'), ('in', 'IN'), ('Oregon', 'NNP'), ('i.e', 'NN'), ('.', '.'), ('every', 'DT'), ('job', 'NN'), ('.', '.'), ('We', 'PRP'), ('will', 'MD'), ('now', 'RB'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('help', 'VB'), ('direct', 'VB'), ('all', 'DT'), ('workers', 'NNS'), ('’', 'VBP'), ('actions', 'NNS'), ('and', 'CC'), ('behavior', 'NN'), ('with', 'IN'), ('a', 'DT'), ('new', 'JJ'), ('degree', 'NN'), ('of', 'IN'), ('intelligence', 'NN'), ('that', 'WDT'), ('comes', 'VBZ'), ('from', 'IN'), ('predictive', 'JJ'), ('analytics', 'NNS'), (',', ','), ('all', 'DT'), ('stemming', 'VBG'), ('from', 'IN'), ('the', 'DT'), ('AI', 'NNP'), ('engines', 'VBZ'), ('we', 'PRP'), ('will', 'MD'), ('now', 'RB'), ('increasingly', 'RB'), ('depend', 'VBP'), ('upon', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "taggedToken=pos_tag(token1)\n",
    "print(taggedToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  And/CC\n",
      "  yes/UH\n",
      "  ,/,\n",
      "  she/PRP\n",
      "  does/VBZ\n",
      "  mean/VB\n",
      "  everybody/NN\n",
      "  's/POS\n",
      "  job/NN\n",
      "  from/IN\n",
      "  yours/NNS\n",
      "  to/TO\n",
      "  mine/VB\n",
      "  and/CC\n",
      "  onward/VB\n",
      "  to/TO\n",
      "  the/DT\n",
      "  role/NN\n",
      "  of/IN\n",
      "  grain/NN\n",
      "  farmers/NNS\n",
      "  in/IN\n",
      "  (GPE Egypt/NNP)\n",
      "  ,/,\n",
      "  pastry/NN\n",
      "  chefs/NNS\n",
      "  in/IN\n",
      "  (GPE Paris/NNP)\n",
      "  and/CC\n",
      "  dog/NN\n",
      "  walkers/NNS\n",
      "  in/IN\n",
      "  (GPE Oregon/NNP)\n",
      "  i.e/NN\n",
      "  ./.\n",
      "  every/DT\n",
      "  job/NN\n",
      "  ./.\n",
      "  We/PRP\n",
      "  will/MD\n",
      "  now/RB\n",
      "  be/VB\n",
      "  able/JJ\n",
      "  to/TO\n",
      "  help/VB\n",
      "  direct/VB\n",
      "  all/DT\n",
      "  workers/NNS\n",
      "  ’/VBP\n",
      "  actions/NNS\n",
      "  and/CC\n",
      "  behavior/NN\n",
      "  with/IN\n",
      "  a/DT\n",
      "  new/JJ\n",
      "  degree/NN\n",
      "  of/IN\n",
      "  intelligence/NN\n",
      "  that/WDT\n",
      "  comes/VBZ\n",
      "  from/IN\n",
      "  predictive/JJ\n",
      "  analytics/NNS\n",
      "  ,/,\n",
      "  all/DT\n",
      "  stemming/VBG\n",
      "  from/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION AI/NNP)\n",
      "  engines/VBZ\n",
      "  we/PRP\n",
      "  will/MD\n",
      "  now/RB\n",
      "  increasingly/RB\n",
      "  depend/VBP\n",
      "  upon/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "neToken=ne_chunk(taggedToken)\n",
    "print(neToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run beauti believ use convers\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "print(\n",
    "ps.stem('running'),\n",
    "ps.stem('beautiful'),\n",
    "ps.stem('believes'),\n",
    "ps.stem('using'),\n",
    "ps.stem('conversation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running beautiful belief using\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem=WordNetLemmatizer()\n",
    "print(\n",
    "lem.lemmatize('running'),\n",
    "lem.lemmatize('beautiful'),\n",
    "lem.lemmatize('believes'),\n",
    "lem.lemmatize('using')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', ','), 3),\n",
       " (('from', 'IN'), 3),\n",
       " (('to', 'TO'), 3),\n",
       " (('and', 'CC'), 3),\n",
       " (('in', 'IN'), 3),\n",
       " (('.', '.'), 3),\n",
       " (('job', 'NN'), 2),\n",
       " (('the', 'DT'), 2),\n",
       " (('of', 'IN'), 2),\n",
       " (('will', 'MD'), 2),\n",
       " (('now', 'RB'), 2),\n",
       " (('all', 'DT'), 2),\n",
       " (('And', 'CC'), 1),\n",
       " (('yes', 'UH'), 1),\n",
       " (('she', 'PRP'), 1),\n",
       " (('does', 'VBZ'), 1),\n",
       " (('mean', 'VB'), 1),\n",
       " (('everybody', 'NN'), 1),\n",
       " ((\"'s\", 'POS'), 1),\n",
       " (('yours', 'NNS'), 1),\n",
       " (('mine', 'VB'), 1),\n",
       " (('onward', 'VB'), 1),\n",
       " (('role', 'NN'), 1),\n",
       " (('grain', 'NN'), 1),\n",
       " (('farmers', 'NNS'), 1),\n",
       " (('Egypt', 'NNP'), 1),\n",
       " (('pastry', 'NN'), 1),\n",
       " (('chefs', 'NNS'), 1),\n",
       " (('Paris', 'NNP'), 1),\n",
       " (('dog', 'NN'), 1),\n",
       " (('walkers', 'NNS'), 1),\n",
       " (('Oregon', 'NNP'), 1),\n",
       " (('i.e', 'NN'), 1),\n",
       " (('every', 'DT'), 1),\n",
       " (('We', 'PRP'), 1),\n",
       " (('be', 'VB'), 1),\n",
       " (('able', 'JJ'), 1),\n",
       " (('help', 'VB'), 1),\n",
       " (('direct', 'VB'), 1),\n",
       " (('workers', 'NNS'), 1),\n",
       " (('’', 'VBP'), 1),\n",
       " (('actions', 'NNS'), 1),\n",
       " (('behavior', 'NN'), 1),\n",
       " (('with', 'IN'), 1),\n",
       " (('a', 'DT'), 1),\n",
       " (('new', 'JJ'), 1),\n",
       " (('degree', 'NN'), 1),\n",
       " (('intelligence', 'NN'), 1),\n",
       " (('that', 'WDT'), 1),\n",
       " (('comes', 'VBZ'), 1),\n",
       " (('predictive', 'JJ'), 1),\n",
       " (('analytics', 'NNS'), 1),\n",
       " (('stemming', 'VBG'), 1),\n",
       " (('AI', 'NNP'), 1),\n",
       " (('engines', 'VBZ'), 1),\n",
       " (('we', 'PRP'), 1),\n",
       " (('increasingly', 'RB'), 1),\n",
       " (('depend', 'VBP'), 1),\n",
       " (('upon', 'NN'), 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWord=[',']\n",
    "from collections import Counter\n",
    "Counter(taggedToken).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.']\n"
     ]
    }
   ],
   "source": [
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든.\"\n",
    "word_tokens = word_tokenize(example) \n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'mean', 'everybody', \"'s\", 'job', 'yours', 'mine', 'onward', 'role', 'grain', 'farmers', 'Egypt', 'pastry', 'chefs', 'Paris', 'dog', 'walkers', 'Oregon', 'i.e', '.', 'job', '.', 'We', 'now', 'be', 'able', 'help', 'direct', 'workers', 'actions', 'behavior', 'new', 'degree', 'intelligence', 'that', 'predictive', 'analytics', 'stemming', 'AI', 'we', 'now', 'increasingly', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "stopPos=['IN','CC',\"UH\",'TO',\"MD\",'DT','VBZ',\"VBP\"]\n",
    "word=[]\n",
    "for tag in taggedToken:\n",
    "    if tag[1] not in stopPos:\n",
    "        if tag[0] not in stopWord:\n",
    "            word.append(tag[0])\n",
    "print(word)            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
